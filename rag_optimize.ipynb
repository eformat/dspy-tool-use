{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81766f50",
   "metadata": {},
   "source": [
    "[RAG](https://dspy.ai/tutorials/rag/) Using a DSPy Optimizer to improve your RAG prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3b8a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/git/dspy-tool-use/venv/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ujson\n",
    "from dspy.utils import download\n",
    "\n",
    "download(\"https://huggingface.co/dspy/cache/resolve/main/ragqa_arena_tech_corpus.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c1dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/git/dspy-tool-use/venv/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28436 documents. Will encode them below.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 72.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 71.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 112.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 96.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 58.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 81.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 122.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 62.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 51.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 77.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 64.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 69.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 67.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 64.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 69.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 155.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 66.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 61.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 68.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 52.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 73.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 59.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "import ujson\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "max_characters = 6000  # for truncating >99th percentile of documents\n",
    "topk_docs_to_retrieve = 5  # number of documents to retrieve per search query\n",
    "\n",
    "with open(\"ragqa_arena_tech_corpus.jsonl\") as f:\n",
    "    corpus = [ujson.loads(line)['text'][:max_characters] for line in f]\n",
    "    print(f\"Loaded {len(corpus)} documents. Will encode them below.\")\n",
    "\n",
    "# same embedding model we use in our RAG\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=\"cuda\")\n",
    "embedder = dspy.Embedder(model.encode)\n",
    "search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=topk_docs_to_retrieve, brute_force_threshold=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9289b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import openai\n",
    "import os\n",
    "\n",
    "LLM_URL=os.getenv('LLM_URL', 'http://localhost:8080/v1')\n",
    "API_KEY=os.getenv('API_KEY', 'fake')\n",
    "LLM_MODEL=os.getenv('LLM_MODEL', 'openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf')\n",
    "MAX_TOKENS=os.getenv('MAX_TOKENS', 6000)\n",
    "TEMPERATURE=os.getenv('TEMPERATURE', 0.2)\n",
    "dspy.enable_logging()\n",
    "lm = dspy.LM(model=LLM_MODEL,\n",
    "             api_base=LLM_URL,  # ensure this points to your port\n",
    "             api_key=API_KEY,\n",
    "             temperature=TEMPERATURE,\n",
    "             model_type='chat',\n",
    "             stream=False)\n",
    "dspy.configure(lm=lm)\n",
    "#dspy.settings.configure(track_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebafe90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.respond = dspy.ChainOfThought('context, question -> response')\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = search(question).passages\n",
    "        return self.respond(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab8877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning='High Memory and Low Memory are terms used to describe the division of memory space in a Linux system. High Memory refers to the segment of memory that user-space programs can access, while Low Memory is the segment that the Linux kernel can access directly. This division is necessary to prevent user-space applications from accessing kernel-space memory, which could potentially lead to security vulnerabilities.',\n",
       "    response='High Memory and Low Memory are terms used to describe the division of memory space in a Linux system. High Memory refers to the segment of memory that user-space programs can access, while Low Memory is the segment that the Linux kernel can access directly. This division is necessary to prevent user-space applications from accessing kernel-space memory, which could potentially lead to security vulnerabilities. The Linux kernel splits the available memory into two parts: High Memory (user-space) and Low Memory (kernel-space), with the latter being further divided into two sub-segments, HIGHMEM and LOWMEM.'\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:45:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    }
   ],
   "source": [
    "rag = RAG()\n",
    "rag(question=\"what are high memory and low memory on linux?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a578c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-06-03T12:22:46.432884]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `context` (str)\n",
      "2. `question` (str)\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `response` (str)\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## context ## ]]\n",
      "{context}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## response ## ]]\n",
      "{response}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `context`, `question`, produce the fields `response`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## context ## ]]\n",
      "[1] «As far as I remember, High Memory is used for application space and Low Memory for the kernel. Advantage is that (user-space) applications cant access kernel-space memory.»\n",
      "[2] «This is relevant to the Linux kernel; Im not sure how any Unix kernel handles this. The High Memory is the segment of memory that user-space programs can address. It cannot touch Low Memory. Low Memory is the segment of memory that the Linux kernel can address directly. If the kernel must access High Memory, it has to map it into its own address space first. There was a patch introduced recently that lets you control where the segment is. The tradeoff is that you can take addressable memory away from user space so that the kernel can have more memory that it does not have to map before using. Additional resources: http://tldp.org/HOWTO/KernelAnalysis-HOWTO-7.html http://linux-mm.org/HighMemory»\n",
      "[3] «HIGHMEM is a range of kernels memory space, but it is NOT memory you access but its a place where you put what you want to access. A typical 32bit Linux virtual memory map is like: 0x00000000-0xbfffffff: user process (3GB) 0xc0000000-0xffffffff: kernel space (1GB) (CPU-specific vector and whatsoever are ignored here). Linux splits the 1GB kernel space into 2 pieces, LOWMEM and HIGHMEM. The split varies from installation to installation. If an installation chooses, say, 512MB-512MB for LOW and HIGH mems, the 512MB LOWMEM (0xc0000000-0xdfffffff) is statically mapped at the kernel boot time; usually the first so many bytes of the physical memory is used for this so that virtual and physical addresses in this range have a constant offset of, say, 0xc0000000. On the other hand, the latter 512MB (HIGHMEM) has no static mapping (although you could leave pages semi-permanently mapped there, but you must do so explicitly in your driver code). Instead, pages are temporarily mapped and unmapped here so that virtual and physical addresses in this range have no consistent mapping. Typical uses of HIGHMEM include single-time data buffers.»\n",
      "[4] «On a 32-bit architecture, the address space range for addressing RAM is: 0x00000000 - 0xffffffff or 4294967295 (4 GB). The linux kernel splits that up 3/1 (could also be 2/2, or 1/3 1) into user space (high memory) and kernel space (low memory) respectively. The user space range: 0x00000000 - 0xbfffffff Every newly spawned user process gets an address (range) inside this area. User processes are generally untrusted and therefore are forbidden to access the kernel space. Further, they are considered non-urgent, as a general rule, the kernel tries to defer the allocation of memory to those processes. The kernel space range: 0xc0000000 - 0xffffffff A kernel processes gets its address (range) here. The kernel can directly access this 1 GB of addresses (well, not the full 1 GB, there are 128 MB reserved for high memory access). Processes spawned in kernel space are trusted, urgent and assumed error-free, the memory request gets processed instantaneously. Every kernel process can also access the user space range if it wishes to. And to achieve this, the kernel maps an address from the user space (the high memory) to its kernel space (the low memory), the 128 MB mentioned above are especially reserved for this. 1 Whether the split is 3/1, 2/2, or 1/3 is controlled by the CONFIG_VMSPLIT_... option; you can probably check under /boot/config* to see which option was selected for your kernel.»\n",
      "[5] «It may be a huge doc to start, but I think its worth the time youll need to read it : Have look on the Linux-Insides doc, more precisely the Memory Management part. You can also read it through Gitbooks Have fun.»\n",
      "\n",
      "[[ ## question ## ]]\n",
      "what are high memory and low memory on linux?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "High Memory and Low Memory are terms used to describe the division of memory space in a Linux system. High Memory refers to the segment of memory that user-space programs can access, while Low Memory is the segment that the Linux kernel can access directly. This division is necessary to prevent user-space applications from accessing kernel-space memory, which could potentially lead to security vulnerabilities.\n",
      "\n",
      "[[ ## response ## ]]\n",
      "High Memory and Low Memory are terms used to describe the division of memory space in a Linux system. High Memory refers to the segment of memory that user-space programs can access, while Low Memory is the segment that the Linux kernel can access directly. This division is necessary to prevent user-space applications from accessing kernel-space memory, which could potentially lead to security vulnerabilities. The Linux kernel splits the available memory into two parts: High Memory (user-space) and Low Memory (kernel-space), with the latter being further divided into two sub-segments, HIGHMEM and LOWMEM.\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a89a23a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 100, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import ujson\n",
    "\n",
    "with open(\"ragqa_arena_tech_examples.jsonl\") as f:\n",
    "    data = [ujson.loads(line) for line in f]\n",
    "\n",
    "data = [dspy.Example(**d).with_inputs('question') for d in data]\n",
    "\n",
    "random.Random(0).shuffle(data)\n",
    "# 200, 200:500, 500:1000\n",
    "trainset, devset, testset = data[:50], data[50:150], data[150:450]\n",
    "\n",
    "len(trainset), len(devset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aacf90cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 55.82 / 100 (55.8%): 100%|██████████| 100/100 [12:18<00:00,  7.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 12:40:10 INFO dspy.evaluate.evaluate: Average Metric: 55.81709025389931 / 100 (55.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>example_response</th>\n",
       "      <th>gold_doc_ids</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>pred_response</th>\n",
       "      <th>SemanticF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>does using == in javascript ever make sense?</td>\n",
       "      <td>Yes, using `==` in JavaScript can make sense and is convenient in ...</td>\n",
       "      <td>[5778, 5791, 5818]</td>\n",
       "      <td>The use of `==` in JavaScript can be misleading due to its behavio...</td>\n",
       "      <td>Yes, using `==` in JavaScript can make sense in certain situations...</td>\n",
       "      <td>✔️ [0.667]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is the difference between a virus and trojan?</td>\n",
       "      <td>The terms have a great deal of overlap and aren't necessarily mutu...</td>\n",
       "      <td>[3768, 3769, 3888, 3890, 4046]</td>\n",
       "      <td>The difference between a virus and a Trojan lies in how they sprea...</td>\n",
       "      <td>A virus and a Trojan are both types of malware, but they differ in...</td>\n",
       "      <td>✔️ [0.600]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0        does using == in javascript ever make sense?   \n",
       "1  what is the difference between a virus and trojan?   \n",
       "\n",
       "                                                        example_response  \\\n",
       "0  Yes, using `==` in JavaScript can make sense and is convenient in ...   \n",
       "1  The terms have a great deal of overlap and aren't necessarily mutu...   \n",
       "\n",
       "                     gold_doc_ids  \\\n",
       "0              [5778, 5791, 5818]   \n",
       "1  [3768, 3769, 3888, 3890, 4046]   \n",
       "\n",
       "                                                               reasoning  \\\n",
       "0  The use of `==` in JavaScript can be misleading due to its behavio...   \n",
       "1  The difference between a virus and a Trojan lies in how they sprea...   \n",
       "\n",
       "                                                           pred_response  \\\n",
       "0  Yes, using `==` in JavaScript can make sense in certain situations...   \n",
       "1  A virus and a Trojan are both types of malware, but they differ in...   \n",
       "\n",
       "   SemanticF1  \n",
       "0  ✔️ [0.667]  \n",
       "1  ✔️ [0.600]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div style='\n",
       "                text-align: center;\n",
       "                font-size: 16px;\n",
       "                font-weight: bold;\n",
       "                color: #555;\n",
       "                margin: 10px 0;'>\n",
       "                ... 98 more rows not displayed ...\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "55.82"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dspy.evaluate import SemanticF1\n",
    "\n",
    "# Instantiate the metric.\n",
    "metric = SemanticF1(decompositional=True)\n",
    "\n",
    "# Define an evaluator that we can re-use.\n",
    "evaluate = dspy.Evaluate(devset=devset, metric=metric, num_threads=24,\n",
    "                         display_progress=True, display_table=2)\n",
    "\n",
    "evaluate(RAG())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3971b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 12:44:43 INFO mlflow.tracking.fluent: Experiment with name 'optimize-rag' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5500\")\n",
    "mlflow.set_experiment(\"optimize-rag\")\n",
    "mlflow.dspy.autolog(\n",
    "    log_compiles=True,    # Track optimization process\n",
    "    log_evals=True,       # Track evaluation results\n",
    "    log_traces_from_compile=True  # Track program traces during optimization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0da1a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 12:45:14 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'a3b3616e10834427b5ac20b2cd63de46', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current dspy workflow\n",
      "2025/06/03 12:45:14 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "RUNNING WITH THE FOLLOWING MEDIUM AUTO RUN SETTINGS:\n",
      "num_trials: 18\n",
      "minibatch: False\n",
      "num_fewshot_candidates: 12\n",
      "num_instruct_candidates: 6\n",
      "valset size: 40\n",
      "\n",
      "2025/06/03 12:45:14 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "2025/06/03 12:45:14 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/06/03 12:45:14 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=12 sets of demonstrations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 1/12\n",
      "Bootstrapping set 2/12\n",
      "Bootstrapping set 3/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 222.11it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[92m12:45:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:45:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:45:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:45:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:45:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 10%|█         | 1/10 [00:16<02:30, 16.75s/it]\u001b[92m12:45:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:45:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:45:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:45:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:45:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 20%|██        | 2/10 [00:31<02:06, 15.85s/it]\u001b[92m12:45:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:45:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:45:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:45:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:45:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:45:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 30%|███       | 3/10 [00:41<01:29, 12.78s/it]\u001b[92m12:45:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:46:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:46:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:46:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:46:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 40%|████      | 4/10 [00:58<01:28, 14.73s/it]\u001b[92m12:46:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:46:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:46:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:46:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:46:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 50%|█████     | 5/10 [01:09<01:05, 13.20s/it]\u001b[92m12:46:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:46:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:46:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:46:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:46:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:46:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 60%|██████    | 6/10 [01:43<01:21, 20.36s/it]\u001b[92m12:46:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:47:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:47:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:47:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:47:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 70%|███████   | 7/10 [01:56<00:53, 17.89s/it]\u001b[92m12:47:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:47:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:47:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:47:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:47:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 80%|████████  | 8/10 [02:13<00:33, 16.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 8 examples for up to 1 rounds, amounting to 8 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 101.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 4/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 173.67it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[92m12:47:28 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:47:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:47:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:32 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:47:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:47:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 10%|█         | 1/10 [00:08<01:16,  8.51s/it]\u001b[92m12:47:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:47:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:47:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:44 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:47:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:47:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 20%|██        | 2/10 [00:22<01:30, 11.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 449.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 5/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 162.38it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[92m12:47:50 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:47:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:47:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:47:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:48:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:48:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 10%|█         | 1/10 [00:11<01:42, 11.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 297.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 6/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 181.20it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[92m12:48:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:48:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:48:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:48:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:48:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 10%|█         | 1/10 [00:14<02:11, 14.64s/it]\u001b[92m12:48:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:48:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:48:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:48:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:48:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 20%|██        | 2/10 [00:30<02:01, 15.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 68.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 7/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 202.64it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[92m12:48:32 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:48:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:48:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:48:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:48:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 10%|█         | 1/10 [00:17<02:40, 17.80s/it]\u001b[92m12:48:50 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:48:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:48:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:48:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:49:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:49:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 20%|██        | 2/10 [00:27<01:49, 13.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 127.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 8/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 147.57it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[92m12:49:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:49:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:49:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:49:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:49:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 10%|█         | 1/10 [00:12<01:50, 12.23s/it]\u001b[92m12:49:12 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:49:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:49:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:49:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:49:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 20%|██        | 2/10 [00:43<02:52, 21.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 180.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 9/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 175.23it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[92m12:49:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:49:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:49:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:49:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:49:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:49:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 10%|█         | 1/10 [00:14<02:09, 14.40s/it]\u001b[92m12:49:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 20%|██        | 2/10 [00:24<01:36, 12.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 132.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 10/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 224.02it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[92m12:50:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 10%|█         | 1/10 [00:33<05:02, 33.59s/it]\u001b[92m12:50:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 20%|██        | 2/10 [00:46<02:52, 21.54s/it]\u001b[92m12:50:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:50:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:50:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:50:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 30%|███       | 3/10 [00:56<02:11, 18.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 173.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 11/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 194.04it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[92m12:51:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 10%|█         | 1/10 [00:27<04:08, 27.61s/it]\u001b[92m12:51:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 20%|██        | 2/10 [00:40<02:33, 19.22s/it]\u001b[92m12:51:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:51:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:51:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:51:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:52:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:52:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 30%|███       | 3/10 [01:05<02:31, 21.65s/it]\u001b[92m12:52:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:52:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:52:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:52:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:52:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 50%|█████     | 5/10 [01:18<01:18, 15.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 5 examples for up to 1 rounds, amounting to 5 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 185.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 12/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 157.42it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[92m12:52:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:52:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:52:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:52:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:52:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      " 10%|█         | 1/10 [00:11<01:40, 11.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 218.42it/s]\n",
      "2025/06/03 12:52:34 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "2025/06/03 12:52:34 INFO dspy.teleprompt.mipro_optimizer_v2: We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n",
      "\u001b[92m12:52:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:52:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:52:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:52:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:52:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "2025/06/03 12:52:43 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "Proposing N=6 instructions...\n",
      "\n",
      "\u001b[92m12:52:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:52:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:52:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:50 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:52:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:52:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:52:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:52:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:52:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:53:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:53:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:53:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:53:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "2025/06/03 12:53:10 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "\u001b[92m12:53:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:53:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:53:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:53:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:53:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:53:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:53:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:53:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:53:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "2025/06/03 12:53:37 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "\u001b[92m12:53:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:53:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:53:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:53:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:53:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "2025/06/03 12:53:45 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "\u001b[92m12:53:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:53:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:53:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:53:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:53:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:53:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:54:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:54:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:54:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:54:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:54:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:54:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:54:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:54:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:30 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:54:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:54:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:54:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:54:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:54:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:55:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:55:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:55:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:55:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "2025/06/03 12:55:14 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 0:\n",
      "\n",
      "2025/06/03 12:55:14 INFO dspy.teleprompt.mipro_optimizer_v2: 0: Given the fields `context`, `question`, produce the fields `response`.\n",
      "\n",
      "2025/06/03 12:55:14 INFO dspy.teleprompt.mipro_optimizer_v2: 1: Given the fields `context`, `question`, produce the fields `response`.\n",
      "\n",
      "2025/06/03 12:55:14 INFO dspy.teleprompt.mipro_optimizer_v2: 2: Use a hash function for verifying the integrity of data, as it provides additional properties such as collision and preimage resistance, making it more suitable for cryptographic applications.\n",
      "\n",
      "2025/06/03 12:55:14 INFO dspy.teleprompt.mipro_optimizer_v2: 3: Given the fields `context`, `question`, produce a response that answers the question and provides relevant information about the topic, including any relevant details or examples that may be helpful in understanding the context.\n",
      "\n",
      "2025/06/03 12:55:14 INFO dspy.teleprompt.mipro_optimizer_v2: 4: Given the context of a user trying to troubleshoot an issue with their Mac OS X device, and the user's concern that their iPhone is not sending messages to their iPad, produce a response that explains how iMessage works and why the iPhone may not be sending messages to the iPad.\n",
      "\n",
      "2025/06/03 12:55:14 INFO dspy.teleprompt.mipro_optimizer_v2: 5: You are a tech-savvy user who needs help understanding the syntax of grep, a powerful command-line tool for searching and manipulating text.\n",
      "\n",
      "2025/06/03 12:55:14 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/06/03 12:55:14 INFO dspy.teleprompt.mipro_optimizer_v2: ==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
      "2025/06/03 12:55:14 INFO dspy.teleprompt.mipro_optimizer_v2: We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
      "\n",
      "2025/06/03 12:55:14 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 1 / 18 - Full Evaluation of Default Program ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:55:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m12:55:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:55:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:55:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:55:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:55:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:55:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:55:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:55:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:55:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:55:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:55:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:30 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:55:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:55:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:55:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:55:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:55:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:55:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:55:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:55:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:55:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:55:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:55:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:55:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:55:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:55:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:55:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:55:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:55:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:56:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:56:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:01 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:56:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:56:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:56:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:56:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:56:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:56:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:56:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:56:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:56:20 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:56:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:56:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:56:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.53 / 1 (53.3%):   2%|▎         | 1/40 [01:09<44:59, 69.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:56:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:56:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:56:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.53 / 2 (26.7%):   5%|▌         | 2/40 [01:11<18:57, 29.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:56:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:56:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:56:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.87 / 3 (28.9%):   8%|▊         | 3/40 [01:16<11:21, 18.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:56:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:56:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:56:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.53 / 4 (38.3%):  10%|█         | 4/40 [01:22<08:14, 13.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:56:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:56:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:56:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.31 / 5 (46.2%):  12%|█▎        | 5/40 [01:25<05:40,  9.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:56:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:56:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:56:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.10 / 6 (51.6%):  15%|█▌        | 6/40 [01:28<04:12,  7.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:56:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:56:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:56:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.92 / 7 (55.9%):  18%|█▊        | 7/40 [01:34<03:47,  6.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:56:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:56:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:56:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.58 / 8 (57.3%):  20%|██        | 8/40 [01:35<02:38,  4.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:56:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:56:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:56:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.20 / 9 (57.7%):  22%|██▎       | 9/40 [01:40<02:42,  5.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:56:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:56:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:56:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:56:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.04 / 10 (60.4%):  25%|██▌       | 10/40 [01:41<01:52,  3.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:56:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:57:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:57:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.71 / 11 (61.0%):  28%|██▊       | 11/40 [01:50<02:39,  5.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:57:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:57:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:57:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.38 / 12 (61.5%):  30%|███       | 12/40 [01:51<01:53,  4.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:57:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:57:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.04 / 13 (61.9%):  32%|███▎      | 13/40 [01:56<01:59,  4.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:57:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:57:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.93 / 14 (63.8%):  35%|███▌      | 14/40 [01:57<01:23,  3.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:57:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:57:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:57:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.72 / 15 (64.8%):  38%|███▊      | 15/40 [02:02<01:34,  3.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:57:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:57:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:57:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.72 / 16 (60.8%):  40%|████      | 16/40 [02:07<01:41,  4.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m12:57:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:57:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:57:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.61 / 17 (62.4%):  42%|████▎     | 17/40 [02:09<01:23,  3.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:57:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:57:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.28 / 18 (62.7%):  45%|████▌     | 18/40 [02:17<01:45,  4.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:57:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:57:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.05 / 19 (63.4%):  48%|████▊     | 19/40 [02:19<01:22,  3.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:57:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:57:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:57:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:57:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:57:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:57:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:57:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:57:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:57:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:57:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:57:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:57:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:57:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:58:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:58:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:58:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:58:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:58:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:58:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:58:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:58:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:58:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:58:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:58:20 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:58:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:58:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:58:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:58:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:58:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:58:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:58:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:58:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:58:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:32 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:58:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:58:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:58:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:58:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:58:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:58:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.05 / 20 (60.3%):  50%|█████     | 20/40 [03:27<07:45, 23.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:58:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:58:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:58:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:58:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.05 / 21 (57.4%):  52%|█████▎    | 21/40 [03:35<05:52, 18.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:58:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:58:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.37 / 22 (56.2%):  55%|█████▌    | 22/40 [03:41<04:27, 14.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:58:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:58:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:58:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.66 / 23 (55.0%):  57%|█████▊    | 23/40 [03:43<03:05, 10.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:59:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:59:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.55 / 24 (56.4%):  60%|██████    | 24/40 [03:49<02:31,  9.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:59:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:59:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.21 / 25 (56.9%):  62%|██████▎   | 25/40 [03:53<01:58,  7.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:59:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:59:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.06 / 26 (57.9%):  65%|██████▌   | 26/40 [04:02<01:54,  8.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:59:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:59:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.73 / 27 (58.2%):  68%|██████▊   | 27/40 [04:23<02:37, 12.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:59:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:59:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.21 / 28 (57.9%):  70%|███████   | 28/40 [04:26<01:51,  9.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:59:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:59:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.21 / 29 (55.9%):  72%|███████▎  | 29/40 [04:31<01:27,  7.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:59:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:59:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.21 / 30 (54.0%):  75%|███████▌  | 30/40 [04:34<01:04,  6.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:59:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:59:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.81 / 31 (54.2%):  78%|███████▊  | 31/40 [04:37<00:50,  5.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:59:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:59:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.47 / 32 (54.6%):  80%|████████  | 32/40 [04:40<00:38,  4.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:59:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m12:59:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m12:59:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.14 / 33 (55.0%):  82%|████████▎ | 33/40 [04:44<00:30,  4.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:00:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:00:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.00 / 34 (55.9%):  85%|████████▌ | 34/40 [04:46<00:23,  3.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:00:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:00:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.60 / 35 (56.0%):  88%|████████▊ | 35/40 [04:50<00:18,  3.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:00:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:00:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.26 / 36 (56.3%):  90%|█████████ | 36/40 [04:52<00:13,  3.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:00:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:00:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.11 / 37 (57.1%):  92%|█████████▎| 37/40 [04:56<00:10,  3.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:00:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:00:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:00:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.89 / 38 (57.6%):  95%|█████████▌| 38/40 [05:02<00:08,  4.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:00:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:00:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:00:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:00:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:00:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:00:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:00:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:00:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:00:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:00:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.89 / 38 (57.6%):  98%|█████████▊| 39/40 [05:22<00:08,  8.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:00:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:00:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.22 / 39 (57.0%): 100%|██████████| 40/40 [05:27<00:00,  7.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:00:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:00:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.89 / 40 (57.2%): : 41it [05:28,  8.02s/it]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 13:00:43 INFO dspy.evaluate.evaluate: Average Metric: 22.885779768497148 / 40 (57.2%)\n",
      "2025/06/03 13:00:43 INFO dspy.teleprompt.mipro_optimizer_v2: Default program score: 57.21\n",
      "\n",
      "/home/mike/git/dspy-tool-use/venv/lib64/python3.12/site-packages/optuna/_experimental.py:31: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "2025/06/03 13:00:43 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 2 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_0 at: http://localhost:5500/#/experiments/344816129373506955/runs/b716a6751d4946e7aa25199dd808db2f\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:00:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:00:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:00:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:00:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:00:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:00:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:00:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:00:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:00:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:00:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:00:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:01:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:01:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:01:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:01:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:01:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:01:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:01:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:01:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:01:20 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:01:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:01:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:01:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:01:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:01:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:28 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:01:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:01:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:01:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:01:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:01:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:01:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:01:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:01:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:01:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:01:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:01:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:01:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:01:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:01:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:01:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:02:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:02:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:02:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:02:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:02:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:02:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:02:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:02:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.75 / 1 (75.0%):   2%|▎         | 1/40 [01:28<57:47, 88.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:02:12 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:02:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:02:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.42 / 2 (70.8%):   5%|▌         | 2/40 [01:32<24:40, 38.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:02:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:02:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:02:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.31 / 3 (76.9%):   8%|▊         | 3/40 [01:38<14:46, 23.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:02:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:02:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:02:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.16 / 4 (79.1%):  10%|█         | 4/40 [01:39<08:48, 14.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:02:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:02:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:02:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.01 / 5 (80.2%):  12%|█▎        | 5/40 [01:45<06:48, 11.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:02:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:02:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:02:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.90 / 6 (81.6%):  15%|█▌        | 6/40 [01:46<04:27,  7.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:02:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:02:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:02:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.67 / 7 (81.0%):  18%|█▊        | 7/40 [01:52<03:57,  7.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:02:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:02:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:02:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.34 / 8 (79.2%):  20%|██        | 8/40 [01:55<03:07,  5.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:02:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:02:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:02:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.91 / 9 (76.8%):  22%|██▎       | 9/40 [01:58<02:40,  5.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:02:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:02:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:02:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.73 / 10 (77.3%):  25%|██▌       | 10/40 [02:01<02:09,  4.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:02:44 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:02:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:02:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.73 / 11 (70.3%):  28%|██▊       | 11/40 [02:05<02:04,  4.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:02:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:02:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:02:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.40 / 12 (70.0%):  30%|███       | 12/40 [02:06<01:30,  3.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:02:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:02:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:02:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.00 / 13 (69.2%):  32%|███▎      | 13/40 [02:12<01:55,  4.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:02:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:02:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:02:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:02:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.84 / 14 (70.3%):  35%|███▌      | 14/40 [02:15<01:37,  3.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:02:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:03:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:03:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.51 / 15 (70.1%):  38%|███▊      | 15/40 [02:19<01:34,  3.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:03:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:03:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:03:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.84 / 16 (67.8%):  40%|████      | 16/40 [02:27<02:00,  5.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:03:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:03:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:03:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.51 / 17 (67.7%):  42%|████▎     | 17/40 [02:28<01:28,  3.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:03:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:03:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.18 / 18 (67.6%):  45%|████▌     | 18/40 [02:33<01:31,  4.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:03:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:03:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.84 / 19 (67.6%):  48%|████▊     | 19/40 [02:34<01:12,  3.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:03:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:03:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:03:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:03:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:30 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:03:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:03:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:03:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:03:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:03:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:03:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:03:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:03:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:03:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:03:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:03:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:03:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:03:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:03:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:03:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:04:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:04:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:04:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:04:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:04:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:04:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:04:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:04:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:04:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:04:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:04:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:04:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:04:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:04:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:28 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:04:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:04:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:30 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:04:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:04:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:04:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:04:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:04:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:04:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.84 / 20 (64.2%):  50%|█████     | 20/40 [04:02<09:34, 28.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:04:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:04:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.51 / 21 (64.3%):  52%|█████▎    | 21/40 [04:04<06:31, 20.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:04:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:04:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.18 / 22 (64.4%):  55%|█████▌    | 22/40 [04:10<04:53, 16.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:04:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:04:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:04:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.50 / 23 (63.0%):  57%|█████▊    | 23/40 [04:13<03:26, 12.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:05:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:05:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.16 / 24 (63.2%):  60%|██████    | 24/40 [04:17<02:37,  9.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:05:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:05:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.83 / 25 (63.3%):  62%|██████▎   | 25/40 [04:19<01:54,  7.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:05:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:05:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.68 / 26 (64.1%):  65%|██████▌   | 26/40 [04:25<01:36,  6.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:05:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:05:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.98 / 27 (62.9%):  68%|██████▊   | 27/40 [04:39<01:59,  9.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:05:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:05:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.83 / 28 (63.7%):  70%|███████   | 28/40 [04:43<01:30,  7.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:05:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:05:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.83 / 29 (61.5%):  72%|███████▎  | 29/40 [04:49<01:17,  7.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:05:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:05:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.40 / 30 (61.3%):  75%|███████▌  | 30/40 [04:53<01:00,  6.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:05:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:05:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.00 / 31 (61.3%):  78%|███████▊  | 31/40 [04:59<00:54,  6.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:05:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:05:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.00 / 32 (59.4%):  80%|████████  | 32/40 [04:59<00:34,  4.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:05:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:05:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.66 / 33 (59.6%):  82%|████████▎ | 33/40 [05:06<00:36,  5.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:05:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:05:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.33 / 34 (59.8%):  85%|████████▌ | 34/40 [05:12<00:31,  5.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:05:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:05:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:05:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.00 / 35 (60.0%):  88%|████████▊ | 35/40 [05:13<00:19,  3.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:06:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:06:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.00 / 36 (58.3%):  90%|█████████ | 36/40 [05:18<00:17,  4.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:06:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:06:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.79 / 37 (58.9%):  92%|█████████▎| 37/40 [05:19<00:10,  3.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:06:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:06:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:06:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:06:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:06:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.63 / 38 (59.6%):  95%|█████████▌| 38/40 [05:27<00:09,  4.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:06:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:06:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:06:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:06:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:06:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:06:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.63 / 38 (59.6%):  98%|█████████▊| 39/40 [05:41<00:07,  7.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:06:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:06:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:06:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:06:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.52 / 39 (60.3%): 100%|██████████| 40/40 [05:50<00:00,  7.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:06:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:06:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.21 / 40 (60.5%): : 41it [05:56,  8.69s/it]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 13:06:39 INFO dspy.evaluate.evaluate: Average Metric: 24.20897977098229 / 40 (60.5%)\n",
      "2025/06/03 13:06:39 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mBest full score so far!\u001b[0m Score: 60.52\n",
      "2025/06/03 13:06:39 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 60.52 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 6'].\n",
      "2025/06/03 13:06:39 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52]\n",
      "2025/06/03 13:06:39 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 60.52\n",
      "2025/06/03 13:06:39 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/06/03 13:06:39 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 3 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_1 at: http://localhost:5500/#/experiments/344816129373506955/runs/a52c98cc281641fda2b36275a0d0956b\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:06:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:06:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:06:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:06:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:06:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:07:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:07:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:01 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:07:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:07:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:07:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:07:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:07:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:07:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:07:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:07:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:07:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:07:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:07:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:07:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:07:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:07:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:07:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:07:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:07:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:08:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:08:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:08:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:08:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:08:20 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:08:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:08:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:08:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:08:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:08:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:08:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:08:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:50 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:08:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:08:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:08:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:09:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:09:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:09:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:09:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:09:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:09:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:09:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:09:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.62 / 1 (61.5%):   2%|▎         | 1/40 [02:36<1:41:49, 156.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:09:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:09:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:09:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.12 / 2 (55.8%):   5%|▌         | 2/40 [02:42<43:08, 68.11s/it]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:09:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:09:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:09:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.78 / 3 (59.4%):   8%|▊         | 3/40 [02:44<23:20, 37.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:09:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:09:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:09:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.45 / 4 (61.2%):  10%|█         | 4/40 [02:49<14:47, 24.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:09:28 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:09:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:09:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.02 / 5 (60.4%):  12%|█▎        | 5/40 [02:49<09:23, 16.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:09:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:09:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:09:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.77 / 6 (62.8%):  15%|█▌        | 6/40 [02:58<07:36, 13.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:09:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:09:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:09:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.54 / 7 (64.9%):  18%|█▊        | 7/40 [03:04<06:01, 10.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:09:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:09:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:09:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.21 / 8 (65.1%):  20%|██        | 8/40 [03:07<04:37,  8.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:09:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:09:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.88 / 9 (65.3%):  22%|██▎       | 9/40 [03:12<03:49,  7.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:09:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:09:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.73 / 10 (67.3%):  25%|██▌       | 10/40 [03:12<02:37,  5.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:09:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:09:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:09:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:09:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.33 / 11 (66.7%):  28%|██▊       | 11/40 [03:19<02:44,  5.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:09:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:10:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:10:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.33 / 12 (61.1%):  30%|███       | 12/40 [03:23<02:21,  5.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:10:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:10:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:10:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.18 / 13 (62.9%):  32%|███▎      | 13/40 [03:29<02:30,  5.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:10:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:10:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:10:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.96 / 14 (64.0%):  35%|███▌      | 14/40 [03:35<02:21,  5.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:10:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:10:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:10:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.89 / 15 (65.9%):  38%|███▊      | 15/40 [03:38<02:03,  4.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:10:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:10:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:10:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.56 / 16 (66.0%):  40%|████      | 16/40 [03:39<01:30,  3.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:10:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:10:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:10:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.22 / 17 (66.0%):  42%|████▎     | 17/40 [03:45<01:36,  4.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:10:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:10:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.11 / 18 (67.3%):  45%|████▌     | 18/40 [03:49<01:33,  4.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:10:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:10:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.59 / 19 (66.3%):  48%|████▊     | 19/40 [03:56<01:46,  5.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:10:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:10:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:10:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:11:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:11:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:11:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:11:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:11:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:11:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:11:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:11:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:11:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:11:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:11:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:11:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:11:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:12:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:12:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:12:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:12:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:12:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:12:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:12:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:12:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:12:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:13:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:13:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:13:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:13:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:13:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:13:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:13:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:13:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:13:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:13:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:13:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:13:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:13:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:14:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:14:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:14:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:14:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "2025/06/03 13:14:16 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.2)  if the reason for truncation is repetition.\n",
      "2025/06/03 13:14:16 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "\u001b[92m13:14:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:14:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:14:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.59 / 20 (63.0%):  50%|█████     | 20/40 [07:39<23:31, 70.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:14:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:14:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.59 / 21 (60.0%):  52%|█████▎    | 21/40 [07:42<15:54, 50.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:14:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:14:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.26 / 22 (60.3%):  55%|█████▌    | 22/40 [07:50<11:17, 37.65s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:14:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:14:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.87 / 23 (60.3%):  57%|█████▊    | 23/40 [07:55<07:52, 27.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:14:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:14:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.87 / 24 (57.8%):  60%|██████    | 24/40 [07:57<05:19, 19.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:14:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:14:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.72 / 25 (58.9%):  62%|██████▎   | 25/40 [08:05<04:07, 16.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:14:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:14:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:14:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.57 / 26 (59.9%):  65%|██████▌   | 26/40 [08:13<03:15, 13.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:15:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:15:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:15:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.57 / 28 (59.2%):  68%|██████▊   | 27/40 [08:23<02:44, 12.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:15:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:15:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.23 / 29 (59.4%):  72%|███████▎  | 29/40 [08:29<01:29,  8.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:15:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:15:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.23 / 30 (57.4%):  75%|███████▌  | 30/40 [08:31<01:05,  6.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:15:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:15:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.90 / 31 (57.7%):  78%|███████▊  | 31/40 [08:36<00:55,  6.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:15:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.76 / 32 (58.6%):  80%|████████  | 32/40 [08:38<00:42,  5.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:15:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:15:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.42 / 33 (58.9%):  82%|████████▎ | 33/40 [08:41<00:32,  4.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:15:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:15:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.21 / 34 (59.5%):  85%|████████▌ | 34/40 [08:47<00:29,  4.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:15:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:15:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.88 / 35 (59.7%):  88%|████████▊ | 35/40 [08:47<00:17,  3.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:15:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:15:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.66 / 36 (60.2%):  90%|█████████ | 36/40 [08:53<00:16,  4.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:15:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:15:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.32 / 37 (60.3%):  92%|█████████▎| 37/40 [08:55<00:10,  3.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 13:15:35 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=4000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.2)  if the reason for truncation is repetition.\n",
      "2025/06/03 13:15:35 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "\u001b[92m13:15:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:15:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:15:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:15:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:15:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:44 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:15:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:15:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:15:53 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:16:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:16:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:16:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:16:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:16:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:16:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:16:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:16:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:16:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:16:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.99 / 38 (60.5%):  95%|█████████▌| 38/40 [09:50<00:37, 18.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:16:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:16:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.78 / 39 (61.0%):  98%|█████████▊| 39/40 [09:53<00:14, 14.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:16:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:16:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.44 / 40 (61.1%): 100%|██████████| 40/40 [10:01<00:00, 15.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 13:16:40 INFO dspy.evaluate.evaluate: Average Metric: 24.444713638682302 / 40 (61.1%)\n",
      "2025/06/03 13:16:40 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mBest full score so far!\u001b[0m Score: 61.11\n",
      "2025/06/03 13:16:40 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 61.11 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 2'].\n",
      "2025/06/03 13:16:40 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52, 61.11]\n",
      "2025/06/03 13:16:40 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 61.11\n",
      "2025/06/03 13:16:40 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/06/03 13:16:40 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 4 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_2 at: http://localhost:5500/#/experiments/344816129373506955/runs/362936e7d50f4b74bf904217a91f0c56\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:16:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:16:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.34 / 40 (60.9%): 100%|██████████| 40/40 [00:05<00:00,  7.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 13:16:46 INFO dspy.evaluate.evaluate: Average Metric: 24.341447303449822 / 40 (60.9%)\n",
      "2025/06/03 13:16:46 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 60.85 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 6'].\n",
      "2025/06/03 13:16:46 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52, 61.11, 60.85]\n",
      "2025/06/03 13:16:46 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 61.11\n",
      "2025/06/03 13:16:46 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/06/03 13:16:46 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 5 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_3 at: http://localhost:5500/#/experiments/344816129373506955/runs/089548dd373b4bc59e68cec9dbca9521\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:16:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:16:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:16:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:16:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:16:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:16:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:16:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:16:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:16:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:17:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:17:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:17:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:17:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:17:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:17:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:17:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:17:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:17:20 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:17:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:17:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:17:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:17:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:17:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:32 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:17:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:17:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:17:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:17:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:17:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:17:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:17:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:17:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:17:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:17:53 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:18:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:18:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:18:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:18:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:18:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:18:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:18:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:18:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:18:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:18:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:18:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:18:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:18:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:18:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.67 / 1 (66.7%):   2%|▎         | 1/40 [01:46<1:09:23, 106.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:18:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:18:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:18:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.33 / 2 (66.7%):   5%|▌         | 2/40 [01:50<29:11, 46.10s/it]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:18:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:18:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:18:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.33 / 3 (44.4%):   8%|▊         | 3/40 [01:56<17:10, 27.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:18:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:18:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:18:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.24 / 4 (56.1%):  10%|█         | 4/40 [01:57<10:13, 17.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:18:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:18:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.91 / 5 (58.2%):  12%|█▎        | 5/40 [02:01<07:20, 12.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:18:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:18:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:18:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.68 / 6 (61.4%):  15%|█▌        | 6/40 [02:03<05:01,  8.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:18:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:18:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:18:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.35 / 7 (62.1%):  18%|█▊        | 7/40 [02:07<03:59,  7.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:18:53 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:18:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:18:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:18:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.24 / 8 (65.5%):  20%|██        | 8/40 [02:13<03:42,  6.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:19:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:19:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:19:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.13 / 9 (68.1%):  22%|██▎       | 9/40 [02:13<02:29,  4.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:19:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:19:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:19:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.73 / 10 (67.3%):  25%|██▌       | 10/40 [02:22<03:01,  6.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:19:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:19:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:19:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:12 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:19:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:19:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.50 / 11 (68.2%):  28%|██▊       | 11/40 [02:38<04:19,  8.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:19:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:19:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:19:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.17 / 12 (68.1%):  30%|███       | 12/40 [02:38<02:58,  6.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:19:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:19:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:19:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.77 / 13 (67.5%):  32%|███▎      | 13/40 [02:43<02:41,  6.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:19:30 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:19:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:19:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.44 / 14 (67.4%):  35%|███▌      | 14/40 [02:45<02:05,  4.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:19:32 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:19:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:19:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.22 / 15 (68.2%):  38%|███▊      | 15/40 [02:53<02:21,  5.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:19:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:19:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:19:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.00 / 16 (68.7%):  40%|████      | 16/40 [02:56<01:59,  4.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:19:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:19:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:19:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.67 / 17 (68.6%):  42%|████▎     | 17/40 [03:08<02:42,  7.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:19:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:19:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:19:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.44 / 18 (69.1%):  45%|████▌     | 18/40 [03:11<02:04,  5.65s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:20:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:20:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:20:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:20:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:20:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:20:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:20:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:20:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:20:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:20:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:20:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:20:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:30 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:20:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:20:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.11 / 19 (69.0%):  48%|████▊     | 19/40 [03:51<05:35, 16.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:20:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:20:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:20:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:20:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:20:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:20:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:20:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:20:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:20:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:21:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:21:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:21:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.79 / 20 (69.0%):  50%|█████     | 20/40 [04:21<06:46, 20.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:21:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:21:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:21:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:21:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:21:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:21:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:21:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:21:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:21:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:21:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:21:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:21:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.79 / 21 (65.7%):  52%|█████▎    | 21/40 [04:55<07:41, 24.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:21:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.79 / 22 (62.7%):  55%|█████▌    | 22/40 [04:58<05:23, 17.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:21:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:21:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.68 / 23 (63.8%):  57%|█████▊    | 23/40 [05:02<03:54, 13.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:21:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:21:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.16 / 24 (63.2%):  60%|██████    | 24/40 [05:05<02:49, 10.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:21:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:21:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.83 / 25 (63.3%):  62%|██████▎   | 25/40 [05:11<02:19,  9.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:21:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:21:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:21:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.49 / 26 (63.4%):  65%|██████▌   | 26/40 [05:12<01:32,  6.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:22:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:22:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.16 / 27 (63.6%):  68%|██████▊   | 27/40 [05:20<01:31,  7.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:22:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:22:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:22:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:22:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.73 / 28 (63.3%):  70%|███████   | 28/40 [05:27<01:25,  7.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:22:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:22:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.40 / 29 (63.4%):  72%|███████▎  | 29/40 [05:36<01:24,  7.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:22:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:22:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.89 / 30 (63.0%):  75%|███████▌  | 30/40 [05:39<01:03,  6.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:22:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:22:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.56 / 31 (63.1%):  78%|███████▊  | 31/40 [05:44<00:52,  5.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:22:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:22:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:22:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:22:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.56 / 32 (61.1%):  80%|████████  | 32/40 [05:51<00:50,  6.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:22:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:22:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.22 / 33 (61.3%):  82%|████████▎ | 33/40 [05:54<00:37,  5.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:22:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:22:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.82 / 34 (61.2%):  85%|████████▌ | 34/40 [06:01<00:34,  5.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:22:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:22:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.67 / 35 (61.9%):  88%|████████▊ | 35/40 [06:03<00:23,  4.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:22:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:22:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:22:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.34 / 36 (62.0%):  90%|█████████ | 36/40 [06:07<00:17,  4.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:23:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:23:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:23:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.11 / 37 (62.5%):  92%|█████████▎| 37/40 [06:15<00:16,  5.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:23:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:23:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:23:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.11 / 38 (60.8%):  95%|█████████▌| 38/40 [06:23<00:12,  6.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:23:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:23:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.78 / 39 (61.0%):  98%|█████████▊| 39/40 [06:25<00:04,  4.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:23:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:23:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.44 / 40 (61.1%): 100%|██████████| 40/40 [06:30<00:00,  9.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 13:23:17 INFO dspy.evaluate.evaluate: Average Metric: 24.443529934276533 / 40 (61.1%)\n",
      "2025/06/03 13:23:17 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 61.11 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 4'].\n",
      "2025/06/03 13:23:17 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52, 61.11, 60.85, 61.11]\n",
      "2025/06/03 13:23:17 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 61.11\n",
      "2025/06/03 13:23:17 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/06/03 13:23:17 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 6 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_4 at: http://localhost:5500/#/experiments/344816129373506955/runs/b0a8cc5613b743f68ec1d7bb5dd1b77d\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:23:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:23:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:23:20 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:23:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:23:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:23:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:23:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:23:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:23:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:23:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:23:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:23:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:23:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:23:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:23:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:23:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:23:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:24:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:24:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:24:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:24:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:24:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:24:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:24:20 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:24:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:24:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:24:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:28 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:24:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:30 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:24:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:24:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:24:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:24:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:24:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:24:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:24:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:24:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:24:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:24:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:24:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:24:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:24:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:25:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:25:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:25:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:25:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.79 / 1 (78.9%):   2%|▎         | 1/40 [01:49<1:10:52, 109.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:25:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:25:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:25:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:25:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.41 / 2 (70.7%):   5%|▌         | 2/40 [02:03<34:00, 53.70s/it]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:25:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:25:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.08 / 3 (69.4%):   8%|▊         | 3/40 [02:04<18:11, 29.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:25:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:25:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:25:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.08 / 4 (52.0%):  10%|█         | 4/40 [02:10<12:04, 20.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:25:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:25:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:25:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.87 / 5 (57.4%):  12%|█▎        | 5/40 [02:13<08:10, 14.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:25:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:25:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:25:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.20 / 6 (53.4%):  15%|█▌        | 6/40 [02:21<06:42, 11.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:25:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:25:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:25:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.09 / 7 (58.5%):  18%|█▊        | 7/40 [02:23<04:45,  8.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:25:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:25:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:25:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.76 / 8 (59.5%):  20%|██        | 8/40 [02:29<04:14,  7.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:25:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:25:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:25:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.61 / 9 (62.3%):  22%|██▎       | 9/40 [02:30<02:59,  5.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:25:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:25:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:25:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.45 / 10 (64.5%):  25%|██▌       | 10/40 [02:36<02:56,  5.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:25:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:25:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:25:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.45 / 11 (58.7%):  28%|██▊       | 11/40 [02:37<02:04,  4.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:25:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:25:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:25:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:25:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.12 / 12 (59.3%):  30%|███       | 12/40 [02:41<01:55,  4.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:25:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:26:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:26:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.79 / 13 (59.9%):  32%|███▎      | 13/40 [02:43<01:37,  3.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:26:01 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:26:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:26:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.45 / 14 (60.4%):  35%|███▌      | 14/40 [02:52<02:13,  5.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:26:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:26:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:26:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.23 / 15 (61.5%):  38%|███▊      | 15/40 [02:54<01:48,  4.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:26:12 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:26:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:26:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.89 / 16 (61.8%):  40%|████      | 16/40 [02:58<01:41,  4.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:26:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:26:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:26:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.56 / 17 (62.1%):  42%|████▎     | 17/40 [03:00<01:20,  3.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:26:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:26:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.34 / 18 (63.0%):  45%|████▌     | 18/40 [03:06<01:29,  4.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:26:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:26:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:28 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:26:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:26:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.11 / 19 (63.7%):  48%|████▊     | 19/40 [03:14<01:55,  5.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:26:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:26:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:26:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:26:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:26:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:26:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:26:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:26:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:26:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:26:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:26:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:27:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:27:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:27:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:27:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:27:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:27:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:27:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:27:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:27:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:27:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:27:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:27:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:27:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:27:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:27:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:27:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:27:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:27:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:27:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:27:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:27:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:27:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:27:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:28:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:28:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:28:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:28:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.11 / 20 (60.5%):  50%|█████     | 20/40 [04:49<10:45, 32.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:28:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:28:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:12 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:28:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:28:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.11 / 21 (57.7%):  52%|█████▎    | 21/40 [04:58<08:00, 25.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:28:20 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:28:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.64 / 22 (57.5%):  55%|█████▌    | 22/40 [05:03<05:46, 19.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:28:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:28:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.93 / 23 (56.2%):  57%|█████▊    | 23/40 [05:07<04:07, 14.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:28:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:28:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.60 / 24 (56.6%):  60%|██████    | 24/40 [05:11<03:03, 11.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:28:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:28:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.60 / 25 (54.4%):  62%|██████▎   | 25/40 [05:15<02:18,  9.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:28:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:28:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.44 / 26 (55.5%):  65%|██████▌   | 26/40 [05:21<01:53,  8.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:28:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:28:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.44 / 27 (53.5%):  68%|██████▊   | 27/40 [05:33<02:02,  9.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:28:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:28:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:28:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.78 / 28 (52.8%):  70%|███████   | 28/40 [05:41<01:45,  8.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:29:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:29:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.78 / 29 (51.0%):  72%|███████▎  | 29/40 [05:49<01:34,  8.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:29:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:29:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.78 / 30 (49.3%):  75%|███████▌  | 30/40 [05:53<01:12,  7.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:29:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:29:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.44 / 31 (49.8%):  78%|███████▊  | 31/40 [05:55<00:50,  5.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:29:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:29:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.04 / 32 (50.1%):  80%|████████  | 32/40 [06:00<00:44,  5.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:29:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:29:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.71 / 33 (50.6%):  82%|████████▎ | 33/40 [06:05<00:37,  5.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:29:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:29:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.60 / 34 (51.8%):  85%|████████▌ | 34/40 [06:12<00:34,  5.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:29:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:29:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:29:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:29:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:29:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:29:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.39 / 35 (52.5%):  88%|████████▊ | 35/40 [06:25<00:39,  7.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:29:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:29:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.28 / 36 (53.5%):  90%|█████████ | 36/40 [06:27<00:24,  6.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:29:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:29:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.94 / 37 (53.9%):  92%|█████████▎| 37/40 [06:33<00:18,  6.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:29:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:29:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:29:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:29:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:29:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:29:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.80 / 38 (54.7%):  95%|█████████▌| 38/40 [06:35<00:09,  4.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:30:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:30:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.65 / 39 (55.5%):  98%|█████████▊| 39/40 [06:44<00:06,  6.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:30:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:30:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.22 / 40 (55.5%): 100%|██████████| 40/40 [06:45<00:00, 10.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 13:30:02 INFO dspy.evaluate.evaluate: Average Metric: 22.218507747432557 / 40 (55.5%)\n",
      "2025/06/03 13:30:02 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 55.55 with parameters ['Predictor 0: Instruction 3', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/06/03 13:30:02 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52, 61.11, 60.85, 61.11, 55.55]\n",
      "2025/06/03 13:30:02 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 61.11\n",
      "2025/06/03 13:30:02 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/06/03 13:30:02 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 7 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_5 at: http://localhost:5500/#/experiments/344816129373506955/runs/464d4d7a7b7145d490ed17e3562ef023\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:30:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:30:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:30:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:30:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:30:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:30:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:30:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:30:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:30:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:30:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:30:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:30:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:30:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:30:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:30:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:30:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:30:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:30:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:30:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:30:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:30:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:30:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:30:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:30:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:30:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:30:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:30:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:30:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:30:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:31:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:31:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:31:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:31:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:31:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:31:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:31:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:31:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:31:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:31:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:31:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:31:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:31:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:32 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:31:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:31:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.89 / 1 (88.9%):   2%|▎         | 1/40 [01:35<1:01:47, 95.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:31:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:31:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:31:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:31:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:31:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.89 / 2 (44.4%):   5%|▌         | 2/40 [01:45<28:48, 45.49s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:31:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:31:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:31:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.50 / 3 (50.1%):   8%|▊         | 3/40 [01:48<16:02, 26.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:31:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:31:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:31:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.17 / 4 (54.3%):  10%|█         | 4/40 [01:53<10:36, 17.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:31:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:31:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:31:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:31:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.84 / 5 (56.8%):  12%|█▎        | 5/40 [01:54<06:42, 11.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:31:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:32:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:32:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.50 / 6 (58.4%):  15%|█▌        | 6/40 [01:58<05:13,  9.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:32:01 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:32:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:32:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.28 / 7 (61.1%):  18%|█▊        | 7/40 [02:05<04:35,  8.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:32:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:32:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:32:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.07 / 8 (63.3%):  20%|██        | 8/40 [02:05<03:06,  5.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:32:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:32:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:32:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.84 / 9 (64.9%):  22%|██▎       | 9/40 [02:11<02:54,  5.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:32:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:32:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:32:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.51 / 10 (65.1%):  25%|██▌       | 10/40 [02:12<02:09,  4.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:32:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:32:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:32:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.18 / 11 (65.2%):  28%|██▊       | 11/40 [02:20<02:37,  5.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:32:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:32:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:32:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.95 / 12 (66.2%):  30%|███       | 12/40 [02:21<01:56,  4.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:32:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:32:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:32:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.72 / 13 (67.1%):  32%|███▎      | 13/40 [02:27<02:04,  4.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:32:30 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:32:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:32:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.57 / 14 (68.4%):  35%|███▌      | 14/40 [02:28<01:35,  3.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:32:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:32:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:32:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.24 / 15 (68.3%):  38%|███▊      | 15/40 [02:33<01:38,  3.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:32:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:32:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:32:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.57 / 16 (66.1%):  40%|████      | 16/40 [02:35<01:23,  3.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:32:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:32:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:32:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.35 / 17 (66.7%):  42%|████▎     | 17/40 [02:39<01:22,  3.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:32:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:32:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.92 / 18 (66.2%):  45%|████▌     | 18/40 [02:42<01:11,  3.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:32:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:32:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:32:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:32:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.67 / 19 (66.7%):  48%|████▊     | 19/40 [02:51<01:46,  5.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:32:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:32:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:32:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:33:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:33:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:33:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:33:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:33:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:33:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:12 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:33:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:33:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:33:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:33:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:33:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:33:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:28 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:33:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:33:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:33:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:33:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:33:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:33:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:33:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:33:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:33:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:33:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:53 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:33:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:33:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:33:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:33:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:33:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.33 / 20 (66.7%):  50%|█████     | 20/40 [03:55<07:37, 22.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:34:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:34:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:34:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:34:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:34:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:34:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:34:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:34:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.33 / 21 (63.5%):  52%|█████▎    | 21/40 [04:13<06:46, 21.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:34:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:34:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.33 / 22 (60.6%):  55%|█████▌    | 22/40 [04:15<04:37, 15.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:34:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:34:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:34:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:34:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.02 / 23 (61.0%):  57%|█████▊    | 23/40 [04:23<03:46, 13.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:34:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:34:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.30 / 24 (59.6%):  60%|██████    | 24/40 [04:27<02:49, 10.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:34:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:34:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.97 / 25 (59.9%):  62%|██████▎   | 25/40 [04:33<02:16,  9.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:34:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:34:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.64 / 26 (60.1%):  65%|██████▌   | 26/40 [04:35<01:38,  7.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:34:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:34:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.49 / 27 (61.1%):  68%|██████▊   | 27/40 [04:46<01:45,  8.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:34:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:34:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.17 / 28 (61.3%):  70%|███████   | 28/40 [04:53<01:34,  7.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:34:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:34:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:34:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.65 / 29 (60.9%):  72%|███████▎  | 29/40 [04:55<01:05,  5.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:35:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:35:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.65 / 30 (58.8%):  75%|███████▌  | 30/40 [05:00<00:58,  5.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:35:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:35:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.22 / 31 (58.8%):  78%|███████▊  | 31/40 [05:04<00:47,  5.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:35:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:35:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.89 / 32 (59.0%):  80%|████████  | 32/40 [05:07<00:35,  4.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:35:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:35:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.78 / 33 (59.9%):  82%|████████▎ | 33/40 [05:14<00:36,  5.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:35:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:35:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.44 / 34 (60.1%):  85%|████████▌ | 34/40 [05:15<00:23,  3.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:35:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:35:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:35:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:35:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.04 / 35 (60.1%):  88%|████████▊ | 35/40 [05:25<00:29,  5.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:35:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:35:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.71 / 36 (60.3%):  90%|█████████ | 36/40 [05:35<00:28,  7.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:35:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:35:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.38 / 37 (60.5%):  92%|█████████▎| 37/40 [05:38<00:17,  5.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:35:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:35:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:35:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:35:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:35:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:35:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:35:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.04 / 38 (60.6%):  95%|█████████▌| 38/40 [05:44<00:11,  5.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:35:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:35:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:53 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:35:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:35:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.71 / 39 (60.8%):  98%|█████████▊| 39/40 [05:53<00:06,  6.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:35:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:35:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:35:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.71 / 39 (60.8%): 100%|██████████| 40/40 [05:55<00:00,  5.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:36:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:36:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.71 / 39 (60.8%): : 41it [06:01,  5.60s/it]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:36:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:36:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.38 / 40 (60.9%): : 42it [06:03,  8.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 13:36:06 INFO dspy.evaluate.evaluate: Average Metric: 24.377706507603552 / 40 (60.9%)\n",
      "2025/06/03 13:36:06 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 60.94 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 6'].\n",
      "2025/06/03 13:36:06 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52, 61.11, 60.85, 61.11, 55.55, 60.94]\n",
      "2025/06/03 13:36:06 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 61.11\n",
      "2025/06/03 13:36:06 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/06/03 13:36:06 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 8 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_6 at: http://localhost:5500/#/experiments/344816129373506955/runs/cbc3af74d4b84445b9d79cf5b8fc74eb\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:36:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:36:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:36:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:36:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:36:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:36:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:36:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:36:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:36:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:36:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:36:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:36:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:36:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:36:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:36:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:36:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:36:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:36:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:36:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:36:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:36:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:36:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:36:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:36:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:37:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:37:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:37:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:37:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:37:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:37:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:37:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:37:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:12 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:37:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:37:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:37:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:37:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:37:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:37:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:37:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:37:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:37:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:37:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:37:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:37:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.85 / 1 (84.7%):   2%|▎         | 1/40 [01:26<56:11, 86.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:37:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:37:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:37:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.53 / 2 (76.6%):   5%|▌         | 2/40 [01:27<22:58, 36.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:37:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:37:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:37:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.20 / 3 (73.3%):   8%|▊         | 3/40 [01:32<13:38, 22.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:37:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:37:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:37:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.87 / 4 (71.7%):  10%|█         | 4/40 [01:35<08:33, 14.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:37:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:37:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:37:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.66 / 5 (73.1%):  12%|█▎        | 5/40 [01:39<06:20, 10.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:37:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:37:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:37:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.32 / 6 (72.0%):  15%|█▌        | 6/40 [01:41<04:23,  7.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:37:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:37:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:37:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.10 / 7 (72.8%):  18%|█▊        | 7/40 [01:48<04:03,  7.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:37:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:37:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:37:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:37:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.87 / 8 (73.4%):  20%|██        | 8/40 [01:51<03:18,  6.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:37:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:38:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.54 / 9 (72.6%):  22%|██▎       | 9/40 [01:54<02:33,  4.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:38:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:38:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.20 / 10 (72.0%):  25%|██▌       | 10/40 [01:58<02:20,  4.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:38:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:38:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.09 / 11 (73.6%):  28%|██▊       | 11/40 [02:00<01:58,  4.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:38:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:38:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.76 / 12 (73.0%):  30%|███       | 12/40 [02:04<01:50,  3.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:38:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:38:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.43 / 13 (72.5%):  32%|███▎      | 13/40 [02:09<01:58,  4.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:38:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:38:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.09 / 14 (72.1%):  35%|███▌      | 14/40 [02:12<01:36,  3.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:38:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:38:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.87 / 15 (72.4%):  38%|███▊      | 15/40 [02:15<01:27,  3.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:38:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:38:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.53 / 16 (72.1%):  40%|████      | 16/40 [02:20<01:38,  4.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:38:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:38:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.20 / 17 (71.8%):  42%|████▎     | 17/40 [02:22<01:22,  3.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:38:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.99 / 18 (72.2%):  45%|████▌     | 18/40 [02:25<01:12,  3.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:38:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.59 / 19 (71.5%):  48%|████▊     | 19/40 [02:28<01:08,  3.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:38:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:38:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:38:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:38:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:50 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:38:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:39:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:01 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:39:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:39:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:39:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:39:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:39:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:39:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:39:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:39:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:39:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:39:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:39:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:39:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.59 / 20 (67.9%):  50%|█████     | 20/40 [03:49<08:50, 26.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:39:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:39:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:39:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:40:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:40:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.59 / 21 (64.7%):  52%|█████▎    | 21/40 [03:55<06:25, 20.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:40:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:40:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:40:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:40:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.28 / 22 (64.9%):  55%|█████▌    | 22/40 [04:02<04:53, 16.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:40:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:40:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.60 / 23 (63.5%):  57%|█████▊    | 23/40 [04:04<03:25, 12.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:40:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:40:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.26 / 24 (63.6%):  60%|██████    | 24/40 [04:10<02:42, 10.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:40:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:40:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.60 / 25 (62.4%):  62%|██████▎   | 25/40 [04:18<02:22,  9.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:40:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:40:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.37 / 26 (63.0%):  65%|██████▌   | 26/40 [04:27<02:13,  9.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:40:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:40:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.85 / 27 (62.4%):  68%|██████▊   | 27/40 [04:29<01:34,  7.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:40:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:40:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.85 / 28 (60.2%):  70%|███████   | 28/40 [04:37<01:30,  7.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:40:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:40:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.42 / 29 (60.1%):  72%|███████▎  | 29/40 [04:45<01:22,  7.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:40:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:40:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.95 / 30 (59.8%):  75%|███████▌  | 30/40 [04:46<00:57,  5.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:40:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:40:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:40:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.62 / 31 (60.1%):  78%|███████▊  | 31/40 [04:50<00:46,  5.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:41:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:41:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.24 / 32 (60.1%):  80%|████████  | 32/40 [04:54<00:37,  4.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:41:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:41:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.13 / 33 (61.0%):  82%|████████▎ | 33/40 [05:00<00:35,  5.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:41:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:41:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.79 / 34 (61.2%):  85%|████████▌ | 34/40 [05:01<00:22,  3.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:41:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:41:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.64 / 35 (61.8%):  88%|████████▊ | 35/40 [05:06<00:21,  4.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:41:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:41:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.24 / 36 (61.8%):  90%|█████████ | 36/40 [05:08<00:14,  3.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.91 / 37 (61.9%):  92%|█████████▎| 37/40 [05:14<00:13,  4.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:41:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:41:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:41:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:41:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:41:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.66 / 38 (62.3%):  95%|█████████▌| 38/40 [05:23<00:11,  5.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:41:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:41:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:41:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:41:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.43 / 39 (62.6%):  98%|█████████▊| 39/40 [05:32<00:06,  6.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:41:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:41:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.43 / 39 (62.6%): 100%|██████████| 40/40 [05:38<00:00,  6.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:41:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:41:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.43 / 39 (62.6%): : 41it [05:41,  5.32s/it]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:41:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:41:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:41:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 25.00 / 40 (62.5%): : 42it [05:51,  8.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 13:41:57 INFO dspy.evaluate.evaluate: Average Metric: 25.001450947336117 / 40 (62.5%)\n",
      "2025/06/03 13:41:57 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mBest full score so far!\u001b[0m Score: 62.5\n",
      "2025/06/03 13:41:57 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 62.5 with parameters ['Predictor 0: Instruction 5', 'Predictor 0: Few-Shot Set 1'].\n",
      "2025/06/03 13:41:57 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52, 61.11, 60.85, 61.11, 55.55, 60.94, 62.5]\n",
      "2025/06/03 13:41:57 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 62.5\n",
      "2025/06/03 13:41:57 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/06/03 13:41:57 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 9 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_7 at: http://localhost:5500/#/experiments/344816129373506955/runs/564c591f36f0441296ff2f2c0341b5c5\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:41:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:41:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:41:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:41:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:41:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:41:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:41:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:41:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:42:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:42:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:42:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:42:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:42:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:42:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:42:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:42:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:42:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:42:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:42:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:42:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:42:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:42:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:42:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:42:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:42:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:42:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:42:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:42:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:42:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:42:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:12 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:42:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:42:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:42:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:42:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:42:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:42:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:42:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:42:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:42:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:42:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:42:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:42:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:42:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:42:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:53 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:42:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:42:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:42:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:43:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:43:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:43:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:43:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:43:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:43:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:43:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:43:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:43:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:43:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:43:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:43:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:43:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:43:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:32 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:43:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:43:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:43:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:43:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:43:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:43:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.67 / 1 (66.7%):   2%|▎         | 1/40 [01:47<1:09:43, 107.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:43:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:43:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:43:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.33 / 2 (66.7%):   5%|▌         | 2/40 [01:53<30:20, 47.92s/it]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:43:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:43:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:43:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.11 / 3 (70.3%):   8%|▊         | 3/40 [01:56<16:45, 27.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:43:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:43:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:43:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:43:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.95 / 4 (73.9%):  10%|█         | 4/40 [02:01<11:11, 18.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:43:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:44:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:44:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.62 / 5 (72.4%):  12%|█▎        | 5/40 [02:02<07:09, 12.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:44:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:44:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:44:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.47 / 6 (74.5%):  15%|█▌        | 6/40 [02:09<05:51, 10.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:44:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:44:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:44:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.47 / 7 (63.8%):  18%|█▊        | 7/40 [02:09<03:56,  7.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:44:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:44:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:44:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.80 / 8 (60.0%):  20%|██        | 8/40 [02:19<04:12,  7.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:44:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:44:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:44:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.62 / 9 (62.4%):  22%|██▎       | 9/40 [02:21<03:08,  6.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:44:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:44:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:44:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:44:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.47 / 11 (58.8%):  25%|██▌       | 10/40 [02:28<03:14,  6.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:44:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:44:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:44:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:44:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.31 / 12 (60.9%):  30%|███       | 12/40 [02:35<02:18,  4.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:44:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:44:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:44:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.16 / 13 (62.8%):  32%|███▎      | 13/40 [02:35<01:44,  3.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:44:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:44:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:44:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.83 / 14 (63.1%):  35%|███▌      | 14/40 [02:40<01:43,  3.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:44:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:44:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:44:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.74 / 15 (64.9%):  38%|███▊      | 15/40 [02:41<01:21,  3.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:44:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:44:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:44:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.40 / 16 (65.0%):  40%|████      | 16/40 [02:45<01:24,  3.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:44:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:44:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:44:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.29 / 17 (66.4%):  42%|████▎     | 17/40 [02:49<01:22,  3.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:44:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:44:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:44:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:44:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.73 / 19 (67.0%):  45%|████▌     | 18/40 [02:55<01:33,  4.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:45:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:45:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:45:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:45:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:45:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:45:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:45:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:45:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:45:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:45:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:45:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:45:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:45:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:45:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:45:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:45:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:45:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:45:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:45:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:45:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:45:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:46:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:46:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:46:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:46:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:46:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:46:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:46:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:46:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:46:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:46:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:46:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:46:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:46:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:46:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:32 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:46:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:46:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:46:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:46:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:46:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:46:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.73 / 20 (63.7%):  50%|█████     | 20/40 [04:53<09:44, 29.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:46:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:46:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:46:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.73 / 21 (60.6%):  52%|█████▎    | 21/40 [04:57<07:14, 22.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:47:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:47:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.05 / 22 (59.3%):  55%|█████▌    | 22/40 [05:03<05:32, 18.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:47:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:47:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.74 / 23 (59.7%):  57%|█████▊    | 23/40 [05:04<03:56, 13.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:47:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:47:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.41 / 24 (60.0%):  60%|██████    | 24/40 [05:11<03:11, 11.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:47:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:47:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.07 / 25 (60.3%):  62%|██████▎   | 25/40 [05:11<02:10,  8.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:47:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:47:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.92 / 26 (61.2%):  65%|██████▌   | 26/40 [05:18<01:51,  7.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:47:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:47:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.60 / 27 (61.5%):  68%|██████▊   | 27/40 [05:28<01:52,  8.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:47:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:47:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.33 / 28 (61.9%):  70%|███████   | 28/40 [05:30<01:21,  6.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:47:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:47:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.33 / 29 (59.8%):  72%|███████▎  | 29/40 [05:36<01:11,  6.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:47:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:47:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.00 / 30 (60.0%):  75%|███████▌  | 30/40 [05:37<00:47,  4.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:47:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:47:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.67 / 31 (60.2%):  78%|███████▊  | 31/40 [05:42<00:43,  4.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:47:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:47:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.24 / 32 (60.1%):  80%|████████  | 32/40 [05:44<00:31,  3.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:47:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:47:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.90 / 33 (60.3%):  82%|████████▎ | 33/40 [05:51<00:34,  4.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:47:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:47:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.57 / 34 (60.5%):  85%|████████▌ | 34/40 [05:51<00:21,  3.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:47:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:47:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.36 / 35 (61.0%):  88%|████████▊ | 35/40 [05:56<00:19,  3.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:47:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:47:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:47:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.96 / 36 (61.0%):  90%|█████████ | 36/40 [05:58<00:13,  3.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:48:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:48:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.63 / 37 (61.2%):  92%|█████████▎| 37/40 [06:04<00:12,  4.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:48:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:48:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:48:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.29 / 38 (61.3%):  95%|█████████▌| 38/40 [06:08<00:08,  4.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:48:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:48:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:48:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:48:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.96 / 39 (61.4%):  98%|█████████▊| 39/40 [06:17<00:05,  5.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:48:20 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:48:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.96 / 39 (61.4%): 100%|██████████| 40/40 [06:22<00:00,  5.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:48:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:48:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:48:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:48:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.96 / 39 (61.4%): : 41it [06:30,  6.15s/it]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:48:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:48:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.85 / 40 (62.1%): : 42it [06:32,  9.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 13:48:30 INFO dspy.evaluate.evaluate: Average Metric: 24.8487274306749 / 40 (62.1%)\n",
      "2025/06/03 13:48:30 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 62.12 with parameters ['Predictor 0: Instruction 3', 'Predictor 0: Few-Shot Set 3'].\n",
      "2025/06/03 13:48:30 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52, 61.11, 60.85, 61.11, 55.55, 60.94, 62.5, 62.12]\n",
      "2025/06/03 13:48:30 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 62.5\n",
      "2025/06/03 13:48:30 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/06/03 13:48:30 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 10 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_8 at: http://localhost:5500/#/experiments/344816129373506955/runs/09fd8d0732314356b294adbb81b6b2e1\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:48:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:48:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:48:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:48:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:48:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:44 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:48:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:48:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:48:53 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:49:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:49:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:49:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:49:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:49:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:49:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:49:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:49:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:12 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:49:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:49:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:49:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:49:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:49:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:49:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:49:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:49:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:49:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:49:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:49:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:49:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:44 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:49:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:49:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:49:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:49:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:49:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:50:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:50:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:50:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:50:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:50:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:50:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:50:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:50:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:50:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:50:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:50:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:50:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.85 / 1 (84.7%):   2%|▎         | 1/40 [01:57<1:16:26, 117.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:50:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:50:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:50:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.70 / 2 (85.2%):   5%|▌         | 2/40 [01:58<30:52, 48.74s/it]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:50:28 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:50:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:50:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.30 / 3 (76.8%):   8%|▊         | 3/40 [02:04<18:04, 29.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:50:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:50:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:50:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.08 / 4 (77.0%):  10%|█         | 4/40 [02:08<11:34, 19.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:50:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:50:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:50:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.08 / 5 (61.6%):  12%|█▎        | 5/40 [02:10<07:37, 13.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:50:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:50:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:50:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.08 / 6 (51.3%):  15%|█▌        | 6/40 [02:18<06:25, 11.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:50:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:50:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:50:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.75 / 7 (53.5%):  18%|█▊        | 7/40 [02:19<04:23,  8.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:50:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:50:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:50:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.41 / 8 (55.1%):  20%|██        | 8/40 [02:27<04:16,  8.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:50:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:50:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:50:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:50:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.26 / 9 (58.4%):  22%|██▎       | 9/40 [02:28<02:58,  5.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:50:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:51:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:51:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.03 / 10 (60.3%):  25%|██▌       | 10/40 [02:35<03:05,  6.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:51:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:51:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:51:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.37 / 11 (57.9%):  28%|██▊       | 11/40 [02:39<02:42,  5.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:51:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:51:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:51:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.26 / 12 (60.5%):  30%|███       | 12/40 [02:41<02:02,  4.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:51:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:51:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.04 / 13 (61.9%):  32%|███▎      | 13/40 [02:47<02:17,  5.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:51:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:51:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.93 / 14 (63.8%):  35%|███▌      | 14/40 [02:48<01:34,  3.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:51:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:51:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.71 / 15 (64.7%):  38%|███▊      | 15/40 [02:57<02:15,  5.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:51:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:51:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:51:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:28 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:51:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:51:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.37 / 16 (64.8%):  40%|████      | 16/40 [03:03<02:15,  5.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:51:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:51:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:51:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.26 / 17 (66.3%):  42%|████▎     | 17/40 [03:06<01:46,  4.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:51:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:51:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.04 / 18 (66.9%):  45%|████▌     | 18/40 [03:15<02:12,  6.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:51:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:51:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:53 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:51:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:51:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:51:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:52:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:52:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:52:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:52:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:52:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:52:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:52:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:52:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:52:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:52:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:52:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:52:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:52:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:52:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:52:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:52:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:52:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:52:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:52:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:52:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:52:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:53:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:53:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.93 / 19 (68.0%):  48%|████▊     | 19/40 [04:32<09:32, 27.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:53:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:53:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:53:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:53:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:53:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:53:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:53:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:53:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.59 / 20 (68.0%):  50%|█████     | 20/40 [04:47<07:54, 23.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:53:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:53:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:53:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:53:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:53:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:53:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.59 / 21 (64.7%):  52%|█████▎    | 21/40 [05:00<06:28, 20.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:53:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:53:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.59 / 22 (61.8%):  55%|█████▌    | 22/40 [05:06<04:52, 16.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:53:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:53:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.91 / 23 (60.5%):  57%|█████▊    | 23/40 [05:11<03:36, 12.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:53:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:53:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.77 / 24 (61.5%):  60%|██████    | 24/40 [05:15<02:41, 10.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:53:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:53:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:53:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.44 / 25 (61.7%):  62%|██████▎   | 25/40 [05:22<02:18,  9.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:54:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:54:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.28 / 26 (62.6%):  65%|██████▌   | 26/40 [05:31<02:09,  9.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:54:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:54:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.95 / 27 (62.8%):  68%|██████▊   | 27/40 [05:33<01:29,  6.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:54:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:54:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.43 / 28 (62.3%):  70%|███████   | 28/40 [05:40<01:23,  6.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:54:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:54:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.43 / 29 (60.1%):  72%|███████▎  | 29/40 [05:49<01:22,  7.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:54:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:54:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.43 / 30 (58.1%):  75%|███████▌  | 30/40 [05:55<01:10,  7.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:54:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:54:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.00 / 31 (58.1%):  78%|███████▊  | 31/40 [05:55<00:46,  5.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:54:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:54:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:30 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:54:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:54:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.67 / 32 (58.3%):  80%|████████  | 32/40 [06:05<00:51,  6.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:54:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:54:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.62 / 33 (59.4%):  82%|████████▎ | 33/40 [06:08<00:39,  5.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:54:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:54:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.62 / 34 (57.7%):  85%|████████▌ | 34/40 [06:15<00:35,  5.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:54:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:54:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.41 / 35 (58.3%):  88%|████████▊ | 35/40 [06:19<00:26,  5.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:54:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:54:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:54:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:54:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:54:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.02 / 36 (58.4%):  90%|█████████ | 36/40 [06:28<00:25,  6.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:55:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:55:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.69 / 37 (58.6%):  92%|█████████▎| 37/40 [06:34<00:19,  6.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:55:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:55:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:55:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:55:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:55:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.46 / 38 (59.1%):  95%|█████████▌| 38/40 [06:40<00:12,  6.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:55:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:55:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.13 / 39 (59.3%):  98%|█████████▊| 39/40 [06:48<00:06,  6.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:55:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:55:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:55:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:55:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.13 / 39 (59.3%): 100%|██████████| 40/40 [06:54<00:00,  6.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:55:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:55:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.13 / 39 (59.3%): : 41it [07:00,  6.21s/it]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:55:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:55:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.90 / 40 (59.8%): : 42it [07:03, 10.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 13:55:34 INFO dspy.evaluate.evaluate: Average Metric: 23.902626381608616 / 40 (59.8%)\n",
      "2025/06/03 13:55:34 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 59.76 with parameters ['Predictor 0: Instruction 3', 'Predictor 0: Few-Shot Set 10'].\n",
      "2025/06/03 13:55:34 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52, 61.11, 60.85, 61.11, 55.55, 60.94, 62.5, 62.12, 59.76]\n",
      "2025/06/03 13:55:34 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 62.5\n",
      "2025/06/03 13:55:34 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/06/03 13:55:34 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 11 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_9 at: http://localhost:5500/#/experiments/344816129373506955/runs/f135822752054ef8967fd98a6b1a3fe7\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "Average Metric: 25.31 / 40 (63.3%): 100%|██████████| 40/40 [00:05<00:00,  7.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 13:55:39 INFO dspy.evaluate.evaluate: Average Metric: 25.308443206605418 / 40 (63.3%)\n",
      "2025/06/03 13:55:39 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mBest full score so far!\u001b[0m Score: 63.27\n",
      "2025/06/03 13:55:39 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 63.27 with parameters ['Predictor 0: Instruction 5', 'Predictor 0: Few-Shot Set 1'].\n",
      "2025/06/03 13:55:39 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52, 61.11, 60.85, 61.11, 55.55, 60.94, 62.5, 62.12, 59.76, 63.27]\n",
      "2025/06/03 13:55:39 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 63.27\n",
      "2025/06/03 13:55:39 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/06/03 13:55:39 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 12 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_10 at: http://localhost:5500/#/experiments/344816129373506955/runs/72f921a93eaf4189a5971029d5609101\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "Average Metric: 25.31 / 40 (63.3%): 100%|██████████| 40/40 [00:05<00:00,  7.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 13:55:45 INFO dspy.evaluate.evaluate: Average Metric: 25.308443206605418 / 40 (63.3%)\n",
      "2025/06/03 13:55:45 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 63.27 with parameters ['Predictor 0: Instruction 5', 'Predictor 0: Few-Shot Set 1'].\n",
      "2025/06/03 13:55:45 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52, 61.11, 60.85, 61.11, 55.55, 60.94, 62.5, 62.12, 59.76, 63.27, 63.27]\n",
      "2025/06/03 13:55:45 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 63.27\n",
      "2025/06/03 13:55:45 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/06/03 13:55:45 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 13 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_11 at: http://localhost:5500/#/experiments/344816129373506955/runs/1093ed0e5bb54c8eabbd5b3f7e280d3f\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:55:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:55:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m13:55:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:55:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:55:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:55:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:55:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:55:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:56:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:56:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:56:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.67 / 1 (66.7%):   2%|▎         | 1/40 [01:20<52:04, 80.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:57:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.24 / 2 (61.9%):   5%|▌         | 2/40 [01:26<23:09, 36.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:57:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.90 / 3 (63.5%):   8%|▊         | 3/40 [01:34<14:29, 23.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:57:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:20 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.79 / 4 (69.8%):  10%|█         | 4/40 [01:35<08:45, 14.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:57:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.46 / 5 (69.2%):  12%|█▎        | 5/40 [01:43<07:18, 12.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:57:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.46 / 6 (57.7%):  15%|█▌        | 6/40 [01:45<04:58,  8.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:57:30 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.13 / 7 (59.0%):  18%|█▊        | 7/40 [01:51<04:21,  7.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:57:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.79 / 8 (59.9%):  20%|██        | 8/40 [01:58<04:00,  7.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:57:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.46 / 9 (60.7%):  22%|██▎       | 9/40 [01:59<02:54,  5.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:57:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.31 / 10 (63.1%):  25%|██▌       | 10/40 [02:04<02:44,  5.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:57:50 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:57:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:57:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:57:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.97 / 11 (63.4%):  28%|██▊       | 11/40 [02:12<02:57,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:57:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.57 / 12 (63.1%):  30%|███       | 12/40 [02:15<02:28,  5.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:58:01 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.35 / 13 (64.2%):  32%|███▎      | 13/40 [02:19<02:08,  4.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:58:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.01 / 14 (64.4%):  35%|███▌      | 14/40 [02:24<02:04,  4.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:58:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.79 / 15 (65.3%):  38%|███▊      | 15/40 [02:26<01:39,  3.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:58:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.68 / 16 (66.7%):  40%|████      | 16/40 [02:31<01:42,  4.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:58:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.68 / 17 (62.8%):  42%|████▎     | 17/40 [02:32<01:16,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.54 / 18 (64.1%):  45%|████▌     | 18/40 [02:40<01:47,  4.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:32 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.38 / 19 (65.2%):  48%|████▊     | 19/40 [02:56<02:51,  8.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:58:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:58:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:59:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:59:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:01 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:59:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:59:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:59:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:59:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:59:20 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:59:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:59:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:59:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:59:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:59:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:59:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:59:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:59:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:59:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:59:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:59:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:53 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:59:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:59:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m13:59:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:00:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:00:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:00:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:00:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:00:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:00:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:00:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:00:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.38 / 20 (61.9%):  50%|█████     | 20/40 [04:30<11:15, 33.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:00:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:00:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.38 / 21 (59.0%):  52%|█████▎    | 21/40 [04:34<07:52, 24.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:00:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:00:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:00:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:00:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.23 / 22 (60.1%):  55%|█████▌    | 22/40 [04:44<06:06, 20.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:00:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:00:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.55 / 23 (58.9%):  57%|█████▊    | 23/40 [04:47<04:16, 15.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:00:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:00:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.22 / 24 (59.2%):  60%|██████    | 24/40 [04:55<03:31, 13.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:00:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:00:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.88 / 25 (59.5%):  62%|██████▎   | 25/40 [04:57<02:24,  9.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:00:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:00:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.73 / 26 (60.5%):  65%|██████▌   | 26/40 [05:03<02:02,  8.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:00:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:00:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:00:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.42 / 27 (60.8%):  68%|██████▊   | 27/40 [05:12<01:54,  8.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.75 / 28 (59.8%):  70%|███████   | 28/40 [05:21<01:44,  8.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.43 / 29 (60.1%):  72%|███████▎  | 29/40 [05:21<01:08,  6.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.10 / 30 (60.3%):  75%|███████▌  | 30/40 [05:26<00:57,  5.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.10 / 31 (58.4%):  78%|███████▊  | 31/40 [05:27<00:39,  4.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.77 / 32 (58.6%):  80%|████████  | 32/40 [05:34<00:41,  5.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.61 / 33 (59.4%):  82%|████████▎ | 33/40 [05:38<00:33,  4.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.40 / 34 (60.0%):  85%|████████▌ | 34/40 [05:40<00:24,  4.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.00 / 35 (60.0%):  88%|████████▊ | 35/40 [05:45<00:20,  4.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.79 / 36 (60.5%):  90%|█████████ | 36/40 [05:48<00:15,  3.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.57 / 37 (61.0%):  92%|█████████▎| 37/40 [05:55<00:14,  4.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:01:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:01:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.85 / 38 (60.1%):  95%|█████████▌| 38/40 [06:05<00:12,  6.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:01:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:01:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:02:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:02:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.85 / 38 (60.1%):  98%|█████████▊| 39/40 [06:18<00:08,  8.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:02:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:02:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.60 / 39 (60.5%): 100%|██████████| 40/40 [06:21<00:00,  6.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:02:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:02:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.45 / 40 (61.1%): : 41it [06:26,  9.42s/it]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 14:02:11 INFO dspy.evaluate.evaluate: Average Metric: 24.450691392820534 / 40 (61.1%)\n",
      "2025/06/03 14:02:11 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 61.13 with parameters ['Predictor 0: Instruction 5', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/06/03 14:02:11 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52, 61.11, 60.85, 61.11, 55.55, 60.94, 62.5, 62.12, 59.76, 63.27, 63.27, 61.13]\n",
      "2025/06/03 14:02:11 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 63.27\n",
      "2025/06/03 14:02:11 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/06/03 14:02:11 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 14 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_12 at: http://localhost:5500/#/experiments/344816129373506955/runs/caf97e68989e48e29ffe677180db4aa1\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:02:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:02:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:02:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:02:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:02:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:02:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:02:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:02:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:02:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:02:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "2025/06/03 14:02:21 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "\u001b[92m14:02:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:02:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:02:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:02:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:02:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:28 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:02:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:02:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:02:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:02:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:02:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:02:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:02:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:02:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:50 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:02:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:02:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:53 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:02:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:02:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:02:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:03:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:01 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:03:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:03:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:03:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:03:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:03:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:03:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:03:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.67 / 1 (66.7%):   2%|▎         | 1/40 [01:15<48:45, 75.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:03:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:03:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:03:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.33 / 2 (66.7%):   5%|▌         | 2/40 [01:25<23:35, 37.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:03:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:03:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 3 (66.7%):   8%|▊         | 3/40 [01:27<13:01, 21.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:03:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:03:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:03:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.89 / 4 (72.2%):  10%|█         | 4/40 [01:40<10:35, 17.65s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:03:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:03:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.78 / 5 (75.6%):  12%|█▎        | 5/40 [01:42<06:59, 11.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:03:53 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:03:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:03:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.06 / 6 (67.7%):  15%|█▌        | 6/40 [01:47<05:32,  9.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:03:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:04:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.85 / 7 (69.3%):  18%|█▊        | 7/40 [01:49<03:54,  7.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:04:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.63 / 8 (70.3%):  20%|██        | 8/40 [01:57<03:58,  7.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:04:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.40 / 9 (71.1%):  22%|██▎       | 9/40 [01:57<02:44,  5.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:04:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.07 / 10 (70.7%):  25%|██▌       | 10/40 [02:04<02:48,  5.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:04:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.73 / 11 (70.3%):  28%|██▊       | 11/40 [02:05<02:08,  4.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:04:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.40 / 12 (70.0%):  30%|███       | 12/40 [02:12<02:22,  5.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:04:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.07 / 13 (69.8%):  32%|███▎      | 13/40 [02:13<01:46,  3.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:04:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.96 / 14 (71.1%):  35%|███▌      | 14/40 [02:19<01:54,  4.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:30 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:04:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.73 / 15 (71.5%):  38%|███▊      | 15/40 [02:23<01:45,  4.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:04:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.58 / 16 (72.4%):  40%|████      | 16/40 [02:30<02:02,  5.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:04:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.15 / 17 (71.5%):  42%|████▎     | 17/40 [02:31<01:33,  4.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:04:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:04:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.82 / 18 (71.2%):  45%|████▌     | 18/40 [02:40<02:01,  5.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:04:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:04:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:04:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:05:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:05:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.48 / 19 (71.0%):  48%|████▊     | 19/40 [02:54<02:48,  8.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:05:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:05:20 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:05:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:05:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:05:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:05:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:05:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:05:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:05:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:50 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:05:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:05:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:06:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:06:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:06:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:06:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:06:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:06:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:06:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:06:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.48 / 20 (67.4%):  50%|█████     | 20/40 [04:05<08:55, 26.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:06:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:06:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.48 / 21 (64.2%):  52%|█████▎    | 21/40 [04:06<06:06, 19.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:06:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:06:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.34 / 22 (65.2%):  55%|█████▌    | 22/40 [04:14<04:42, 15.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:06:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:06:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:28 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:06:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:06:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.66 / 23 (63.7%):  57%|█████▊    | 23/40 [04:23<03:53, 13.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:06:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:06:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:06:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:06:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.99 / 24 (62.5%):  60%|██████    | 24/40 [04:35<03:31, 13.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:06:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:06:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.77 / 25 (63.1%):  62%|██████▎   | 25/40 [04:38<02:30, 10.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:06:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:06:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:06:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.66 / 26 (64.1%):  65%|██████▌   | 26/40 [04:42<01:57,  8.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:07:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:07:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.34 / 27 (64.2%):  68%|██████▊   | 27/40 [04:49<01:44,  8.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:07:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:07:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.01 / 28 (64.3%):  70%|███████   | 28/40 [04:56<01:31,  7.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:07:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:07:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.54 / 29 (63.9%):  72%|███████▎  | 29/40 [04:57<01:02,  5.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:07:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:07:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.54 / 30 (61.8%):  75%|███████▌  | 30/40 [05:04<00:59,  5.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:07:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:07:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.21 / 31 (62.0%):  78%|███████▊  | 31/40 [05:05<00:40,  4.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:07:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:07:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.21 / 32 (60.0%):  80%|████████  | 32/40 [05:10<00:36,  4.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:07:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:07:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.88 / 33 (60.2%):  82%|████████▎ | 33/40 [05:12<00:27,  3.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:07:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:07:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.48 / 34 (60.2%):  85%|████████▌ | 34/40 [05:19<00:29,  4.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:07:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:07:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.14 / 35 (60.4%):  88%|████████▊ | 35/40 [05:26<00:27,  5.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:07:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:07:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:07:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:07:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.00 / 36 (61.1%):  90%|█████████ | 36/40 [05:34<00:25,  6.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:07:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:07:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:07:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:07:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.61 / 37 (61.1%):  92%|█████████▎| 37/40 [05:40<00:18,  6.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:07:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:07:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:07:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:07:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:07:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.19 / 38 (61.0%):  95%|█████████▌| 38/40 [05:45<00:11,  5.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:07:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:07:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:07:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.85 / 39 (61.2%):  98%|█████████▊| 39/40 [05:46<00:04,  4.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:08:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:08:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.52 / 40 (61.3%): 100%|██████████| 40/40 [05:53<00:00,  8.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 14:08:04 INFO dspy.evaluate.evaluate: Average Metric: 24.519484967466592 / 40 (61.3%)\n",
      "2025/06/03 14:08:04 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 61.3 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 1'].\n",
      "2025/06/03 14:08:04 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52, 61.11, 60.85, 61.11, 55.55, 60.94, 62.5, 62.12, 59.76, 63.27, 63.27, 61.13, 61.3]\n",
      "2025/06/03 14:08:04 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 63.27\n",
      "2025/06/03 14:08:04 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/06/03 14:08:04 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 15 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_13 at: http://localhost:5500/#/experiments/344816129373506955/runs/c246f1d59cf042debd80a551ccbc4909\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:08:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:08:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:08:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:08:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:08:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:08:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:08:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:08:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:08:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:08:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:08:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:08:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:08:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:08:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:08:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:08:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:08:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:08:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:08:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:08:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:08:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:08:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:08:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:08:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:08:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:08:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:08:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:53 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:08:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:08:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:08:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.67 / 1 (66.7%):   2%|▎         | 1/40 [01:36<1:02:49, 96.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:09:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.33 / 2 (66.7%):   5%|▌         | 2/40 [01:42<27:26, 43.33s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:09:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 3 (66.7%):   8%|▊         | 3/40 [01:45<15:17, 24.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:09:50 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.67 / 4 (66.7%):  10%|█         | 4/40 [01:48<09:39, 16.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:09:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.33 / 5 (66.7%):  12%|█▎        | 5/40 [01:52<06:49, 11.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:09:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:09:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.81 / 6 (63.6%):  15%|█▌        | 6/40 [01:53<04:40,  8.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:09:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:10:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:10:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.66 / 7 (66.6%):  18%|█▊        | 7/40 [01:58<04:00,  7.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:10:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:10:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:10:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.33 / 8 (66.6%):  20%|██        | 8/40 [02:02<03:11,  5.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:10:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:10:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:10:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.15 / 9 (68.3%):  22%|██▎       | 9/40 [02:07<03:00,  5.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:10:12 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:10:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:10:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.03 / 10 (70.3%):  25%|██▌       | 10/40 [02:08<02:11,  4.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:10:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:10:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:10:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.70 / 11 (70.0%):  28%|██▊       | 11/40 [02:13<02:06,  4.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:10:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:10:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:10:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.47 / 12 (70.6%):  30%|███       | 12/40 [02:16<01:58,  4.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:10:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:10:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:10:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.36 / 13 (72.0%):  32%|███▎      | 13/40 [02:22<02:05,  4.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:10:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:10:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:10:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.03 / 14 (71.6%):  35%|███▌      | 14/40 [02:23<01:35,  3.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:10:28 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:10:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:10:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.03 / 15 (66.9%):  38%|███▊      | 15/40 [02:31<01:59,  4.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:10:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:10:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:10:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.89 / 16 (68.0%):  40%|████      | 16/40 [02:31<01:24,  3.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:10:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:10:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:10:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.49 / 17 (67.6%):  42%|████▎     | 17/40 [02:37<01:33,  4.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:10:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:10:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.06 / 18 (67.0%):  45%|████▌     | 18/40 [02:39<01:16,  3.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:10:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:10:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.67 / 19 (66.7%):  48%|████▊     | 19/40 [02:44<01:26,  4.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:10:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:10:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:10:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:10:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:10:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:11:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:11:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:11:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:11:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:11:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:11:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:11:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:11:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:11:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:11:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:11:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:11:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:11:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:11:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:11:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:11:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:11:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:11:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:44 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:11:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:11:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:11:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:11:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:11:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:11:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:11:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:12:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:12:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:12:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:12:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:12:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:12:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:12:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:12:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:12:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:12:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:12:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:12:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.67 / 20 (63.4%):  50%|█████     | 20/40 [04:16<10:08, 30.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:12:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:12:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.67 / 21 (60.4%):  52%|█████▎    | 21/40 [04:21<07:09, 22.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:12:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:12:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.96 / 22 (58.9%):  55%|█████▌    | 22/40 [04:24<05:04, 16.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:12:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:12:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.44 / 23 (58.4%):  57%|█████▊    | 23/40 [04:31<03:53, 13.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:12:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:12:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.13 / 24 (58.9%):  60%|██████    | 24/40 [04:32<02:40, 10.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:12:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:12:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.97 / 25 (59.9%):  62%|██████▎   | 25/40 [04:40<02:22,  9.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:12:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:12:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.86 / 26 (61.0%):  65%|██████▌   | 26/40 [04:43<01:44,  7.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:12:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:12:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:12:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.40 / 27 (60.7%):  68%|██████▊   | 27/40 [04:52<01:42,  7.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:13:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.40 / 28 (58.6%):  70%|███████   | 28/40 [04:58<01:28,  7.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:13:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.12 / 29 (59.0%):  72%|███████▎  | 29/40 [05:00<01:02,  5.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:13:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.79 / 30 (59.3%):  75%|███████▌  | 30/40 [05:04<00:52,  5.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:13:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.12 / 31 (58.5%):  78%|███████▊  | 31/40 [05:06<00:37,  4.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:13:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.79 / 32 (58.7%):  80%|████████  | 32/40 [05:12<00:37,  4.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:13:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.29 / 33 (58.5%):  82%|████████▎ | 33/40 [05:13<00:26,  3.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:13:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.96 / 34 (58.7%):  85%|████████▌ | 34/40 [05:19<00:25,  4.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:13:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.56 / 35 (58.7%):  88%|████████▊ | 35/40 [05:20<00:16,  3.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:13:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.22 / 36 (59.0%):  90%|█████████ | 36/40 [05:26<00:16,  4.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:13:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.89 / 37 (59.2%):  92%|█████████▎| 37/40 [05:28<00:10,  3.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:13:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:13:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:13:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:13:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.56 / 38 (59.4%):  95%|█████████▌| 38/40 [05:32<00:07,  3.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:13:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.22 / 39 (59.5%):  98%|█████████▊| 39/40 [05:39<00:04,  4.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:13:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:13:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:13:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:13:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:13:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.22 / 39 (59.5%): 100%|██████████| 40/40 [05:55<00:00,  8.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:14:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:14:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.00 / 40 (60.0%): : 41it [05:57,  8.72s/it]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 14:14:02 INFO dspy.evaluate.evaluate: Average Metric: 23.99667723800551 / 40 (60.0%)\n",
      "2025/06/03 14:14:02 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 59.99 with parameters ['Predictor 0: Instruction 5', 'Predictor 0: Few-Shot Set 9'].\n",
      "2025/06/03 14:14:02 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52, 61.11, 60.85, 61.11, 55.55, 60.94, 62.5, 62.12, 59.76, 63.27, 63.27, 61.13, 61.3, 59.99]\n",
      "2025/06/03 14:14:02 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 63.27\n",
      "2025/06/03 14:14:02 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/06/03 14:14:02 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 16 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_14 at: http://localhost:5500/#/experiments/344816129373506955/runs/b85186b8bafa441ba756b1e953301349\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:14:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:14:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:14:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:14:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:14:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:14:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:14:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:14:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:14:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:14:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:14:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:30 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:14:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:14:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:14:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:14:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:14:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:14:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:14:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:14:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:14:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:14:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:15:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:15:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:15:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:15:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:15:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:15:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:12 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:15:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:15:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:15:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:15:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:15:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:15:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:15:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.67 / 1 (66.7%):   2%|▎         | 1/40 [01:23<53:59, 83.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:15:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:15:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:15:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:15:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:15:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:15:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:15:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:15:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:15:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:15:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:15:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.58 / 2 (78.8%):   5%|▌         | 2/40 [01:48<31:18, 49.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:15:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:15:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:15:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:15:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.43 / 3 (81.1%):   8%|▊         | 3/40 [01:52<17:32, 28.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:15:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:16:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:16:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.91 / 4 (72.8%):  10%|█         | 4/40 [01:57<11:33, 19.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:16:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:16:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:16:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.53 / 5 (70.6%):  12%|█▎        | 5/40 [02:04<08:37, 14.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:16:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:16:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:16:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.30 / 6 (71.7%):  15%|█▌        | 6/40 [02:05<05:38,  9.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:16:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:16:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:16:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.97 / 7 (71.0%):  18%|█▊        | 7/40 [02:10<04:40,  8.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:16:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:16:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:16:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.82 / 8 (72.7%):  20%|██        | 8/40 [02:18<04:27,  8.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:16:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:16:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:16:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.66 / 9 (74.0%):  22%|██▎       | 9/40 [02:19<03:06,  6.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:16:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:16:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:16:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.26 / 10 (72.6%):  25%|██▌       | 10/40 [02:25<02:59,  5.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:16:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:16:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:16:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.26 / 11 (66.0%):  28%|██▊       | 11/40 [02:28<02:28,  5.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:16:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:16:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:16:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.83 / 12 (65.3%):  30%|███       | 12/40 [02:31<02:09,  4.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:16:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:16:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:16:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.50 / 13 (65.4%):  32%|███▎      | 13/40 [02:36<02:04,  4.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:16:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:16:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:16:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.17 / 14 (65.5%):  35%|███▌      | 14/40 [02:42<02:08,  4.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:16:44 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:16:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:16:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.83 / 15 (65.6%):  38%|███▊      | 15/40 [02:45<01:47,  4.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:16:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:16:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:16:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:16:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:16:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:16:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.50 / 16 (65.6%):  40%|████      | 16/40 [02:55<02:25,  6.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:16:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:17:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:17:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.17 / 17 (65.7%):  42%|████▎     | 17/40 [03:01<02:20,  6.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:17:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:17:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.02 / 18 (66.8%):  45%|████▌     | 18/40 [03:03<01:47,  4.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:17:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:17:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.90 / 19 (67.9%):  48%|████▊     | 19/40 [03:10<01:56,  5.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:17:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:17:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:17:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:17:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:17:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:17:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:17:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:17:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:17:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:17:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:17:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:17:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:17:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:53 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:17:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:17:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:17:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:18:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:18:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:01 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:18:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:18:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:18:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:18:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:18:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:18:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:18:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:18:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:18:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:18:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.57 / 20 (67.9%):  50%|█████     | 20/40 [04:24<08:41, 26.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:18:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:18:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:32 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:18:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:18:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:18:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:18:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:18:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:18:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:18:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.24 / 21 (67.8%):  52%|█████▎    | 21/40 [04:49<08:06, 25.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:18:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:18:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:18:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:19:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:19:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.13 / 22 (68.8%):  55%|█████▌    | 22/40 [04:58<06:16, 20.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:19:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:19:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.45 / 23 (67.2%):  57%|█████▊    | 23/40 [05:02<04:26, 15.65s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:19:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:19:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.11 / 24 (67.1%):  60%|██████    | 24/40 [05:08<03:25, 12.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:19:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:19:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.78 / 25 (67.1%):  62%|██████▎   | 25/40 [05:10<02:23,  9.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:19:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:19:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.64 / 26 (67.8%):  65%|██████▌   | 26/40 [05:20<02:15,  9.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:19:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:19:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.32 / 27 (67.9%):  68%|██████▊   | 27/40 [05:29<02:03,  9.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:19:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:19:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.86 / 28 (67.3%):  70%|███████   | 28/40 [05:29<01:21,  6.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:19:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:19:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:19:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.52 / 30 (65.1%):  72%|███████▎  | 29/40 [05:36<01:13,  6.71s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:19:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:19:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.19 / 31 (65.1%):  78%|███████▊  | 31/40 [05:43<00:45,  5.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:19:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:19:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.19 / 32 (63.1%):  80%|████████  | 32/40 [05:50<00:45,  5.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:19:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:19:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:19:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.19 / 33 (61.2%):  82%|████████▎ | 33/40 [05:52<00:33,  4.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:20:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:20:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:01 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:20:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:20:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.19 / 34 (59.4%):  85%|████████▌ | 34/40 [06:01<00:35,  5.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:20:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:20:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.79 / 35 (59.4%):  88%|████████▊ | 35/40 [06:06<00:28,  5.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:20:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:20:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.46 / 36 (59.6%):  90%|█████████ | 36/40 [06:08<00:18,  4.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:20:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:20:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.12 / 37 (59.8%):  92%|█████████▎| 37/40 [06:14<00:15,  5.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:20:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:20:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:20:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.79 / 38 (60.0%):  95%|█████████▌| 38/40 [06:17<00:08,  4.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:20:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:20:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:20:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:20:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.46 / 39 (60.1%):  98%|█████████▊| 39/40 [06:32<00:07,  7.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:20:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:20:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:20:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:20:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.46 / 39 (60.1%): 100%|██████████| 40/40 [06:39<00:00,  7.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:20:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:20:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.46 / 39 (60.1%): : 41it [06:40,  5.48s/it]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:20:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:20:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.27 / 40 (60.7%): : 42it [06:45,  9.65s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 14:20:47 INFO dspy.evaluate.evaluate: Average Metric: 24.27390037750569 / 40 (60.7%)\n",
      "2025/06/03 14:20:47 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 60.68 with parameters ['Predictor 0: Instruction 5', 'Predictor 0: Few-Shot Set 8'].\n",
      "2025/06/03 14:20:47 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52, 61.11, 60.85, 61.11, 55.55, 60.94, 62.5, 62.12, 59.76, 63.27, 63.27, 61.13, 61.3, 59.99, 60.68]\n",
      "2025/06/03 14:20:47 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 63.27\n",
      "2025/06/03 14:20:47 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/06/03 14:20:47 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 17 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_15 at: http://localhost:5500/#/experiments/344816129373506955/runs/7074516c57564994b8278c34360ec51a\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:20:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:20:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:20:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:20:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:20:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:20:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:21:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:01 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:21:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:21:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:21:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:21:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:21:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:21:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:21:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:21:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:21:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:21:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:21:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:30 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:21:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:21:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:21:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:21:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:21:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:21:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:49 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:21:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:21:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:21:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:22:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:22:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:22:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:22:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:22:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:22:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:22:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.67 / 1 (66.7%):   2%|▎         | 1/40 [01:25<55:37, 85.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:22:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:22:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:22:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:22:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:22:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.33 / 2 (66.7%):   5%|▌         | 2/40 [01:35<26:09, 41.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:22:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:22:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:22:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:22:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:22:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.33 / 3 (44.4%):   8%|▊         | 3/40 [01:42<15:47, 25.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:22:30 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:22:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:22:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.11 / 4 (52.7%):  10%|█         | 4/40 [01:45<10:02, 16.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:22:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:22:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:22:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.71 / 5 (54.2%):  12%|█▎        | 5/40 [01:50<07:14, 12.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:22:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:22:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:22:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.37 / 6 (56.2%):  15%|█▌        | 6/40 [01:56<05:42, 10.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:22:44 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:22:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:22:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.26 / 7 (60.9%):  18%|█▊        | 7/40 [01:58<04:07,  7.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:22:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:22:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:22:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.11 / 8 (63.9%):  20%|██        | 8/40 [02:02<03:29,  6.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:22:50 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:22:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:22:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:22:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.93 / 9 (65.9%):  22%|██▎       | 9/40 [02:03<02:29,  4.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:22:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:23:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.53 / 10 (65.3%):  25%|██▌       | 10/40 [02:12<03:00,  6.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:23:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:23:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:23:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.19 / 11 (65.4%):  28%|██▊       | 11/40 [02:13<02:10,  4.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:23:01 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:23:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:23:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.97 / 12 (66.5%):  30%|███       | 12/40 [02:22<02:42,  5.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:23:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:23:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:23:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.86 / 13 (68.2%):  32%|███▎      | 13/40 [02:23<01:53,  4.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:23:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:23:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:23:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.65 / 14 (68.9%):  35%|███▌      | 14/40 [02:28<02:02,  4.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:23:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:23:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:23:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.34 / 15 (68.9%):  38%|███▊      | 15/40 [02:30<01:36,  3.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:23:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:23:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:23:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.00 / 16 (68.8%):  40%|████      | 16/40 [02:35<01:41,  4.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:23:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:23:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:23:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:32 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:23:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:23:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.67 / 17 (68.7%):  42%|████▎     | 17/40 [02:45<02:15,  5.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:23:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:23:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.34 / 18 (68.5%):  45%|████▌     | 18/40 [02:55<02:37,  7.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:23:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:23:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:23:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:23:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:23:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.00 / 19 (68.4%):  48%|████▊     | 19/40 [03:09<03:10,  9.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:24:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:24:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:24:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:24:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:24:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:24:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:24:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:24:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:24:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:24:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:24:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:24:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:24:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:24:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:24:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:24:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:24:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:24:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:24:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:24:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:24:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:24:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:25:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:25:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:25:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:25:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:25:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:25:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:25:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:25:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.67 / 20 (68.4%):  50%|█████     | 20/40 [04:25<09:44, 29.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:25:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:25:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.67 / 21 (65.1%):  52%|█████▎    | 21/40 [04:29<06:51, 21.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:25:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:25:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:25:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:25:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.46 / 22 (65.7%):  55%|█████▌    | 22/40 [04:36<05:09, 17.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:25:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:25:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:30 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:25:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:25:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.75 / 23 (64.1%):  57%|█████▊    | 23/40 [04:47<04:20, 15.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:25:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:25:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.41 / 24 (64.2%):  60%|██████    | 24/40 [04:54<03:25, 12.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:25:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:25:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.30 / 25 (65.2%):  62%|██████▎   | 25/40 [04:54<02:17,  9.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:25:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:25:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:25:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.15 / 26 (66.0%):  65%|██████▌   | 26/40 [05:01<01:58,  8.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:26:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:26:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.78 / 27 (65.9%):  68%|██████▊   | 27/40 [05:20<02:31, 11.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:26:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:26:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.63 / 28 (66.5%):  70%|███████   | 28/40 [05:24<01:52,  9.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:26:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:26:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.96 / 29 (65.4%):  72%|███████▎  | 29/40 [05:27<01:21,  7.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:26:20 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:26:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.63 / 30 (65.4%):  75%|███████▌  | 30/40 [05:32<01:06,  6.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:26:20 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:26:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.63 / 31 (63.3%):  78%|███████▊  | 31/40 [05:33<00:43,  4.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:26:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:26:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.30 / 32 (63.4%):  80%|████████  | 32/40 [05:39<00:42,  5.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:26:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:26:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.96 / 33 (63.5%):  82%|████████▎ | 33/40 [05:41<00:29,  4.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:26:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:26:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.75 / 34 (64.0%):  85%|████████▌ | 34/40 [05:48<00:30,  5.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:26:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:26:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.64 / 35 (64.7%):  88%|████████▊ | 35/40 [05:51<00:21,  4.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:26:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:26:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.31 / 36 (64.7%):  90%|█████████ | 36/40 [05:57<00:19,  4.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:26:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:26:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:26:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:26:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:26:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:26:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:26:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.98 / 37 (64.8%):  92%|█████████▎| 37/40 [06:11<00:22,  7.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:26:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:26:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m14:26:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:27:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:27:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.87 / 38 (65.4%):  95%|█████████▌| 38/40 [06:15<00:13,  6.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:27:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:27:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 25.53 / 39 (65.5%):  98%|█████████▊| 39/40 [06:17<00:05,  5.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:27:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:27:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 26.31 / 40 (65.8%): 100%|██████████| 40/40 [06:21<00:00,  9.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 14:27:09 INFO dspy.evaluate.evaluate: Average Metric: 26.306180868190257 / 40 (65.8%)\n",
      "2025/06/03 14:27:09 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mBest full score so far!\u001b[0m Score: 65.77\n",
      "2025/06/03 14:27:09 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 65.77 with parameters ['Predictor 0: Instruction 5', 'Predictor 0: Few-Shot Set 4'].\n",
      "2025/06/03 14:27:09 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52, 61.11, 60.85, 61.11, 55.55, 60.94, 62.5, 62.12, 59.76, 63.27, 63.27, 61.13, 61.3, 59.99, 60.68, 65.77]\n",
      "2025/06/03 14:27:09 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 65.77\n",
      "2025/06/03 14:27:09 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/06/03 14:27:09 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 18 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_16 at: http://localhost:5500/#/experiments/344816129373506955/runs/a68f92246a624aa187e47a692ec4d37a\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "Average Metric: 2.15 / 3 (71.7%):   5%|▌         | 2/40 [00:00<00:11,  3.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:27:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:27:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 26.31 / 40 (65.8%): 100%|██████████| 40/40 [00:06<00:00,  6.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 14:27:15 INFO dspy.evaluate.evaluate: Average Metric: 26.306180868190257 / 40 (65.8%)\n",
      "2025/06/03 14:27:15 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 65.77 with parameters ['Predictor 0: Instruction 5', 'Predictor 0: Few-Shot Set 4'].\n",
      "2025/06/03 14:27:15 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52, 61.11, 60.85, 61.11, 55.55, 60.94, 62.5, 62.12, 59.76, 63.27, 63.27, 61.13, 61.3, 59.99, 60.68, 65.77, 65.77]\n",
      "2025/06/03 14:27:15 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 65.77\n",
      "2025/06/03 14:27:15 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/06/03 14:27:15 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 19 / 18 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_17 at: http://localhost:5500/#/experiments/344816129373506955/runs/15fd7a4cd8ba48d984544c18db0a4325\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n",
      "Average Metric: 1.56 / 2 (77.8%):   2%|▎         | 1/40 [00:00<00:13,  2.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:27:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:27:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:27:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:27:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m14:27:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 26.20 / 40 (65.5%): 100%|██████████| 40/40 [00:06<00:00,  5.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 14:27:22 INFO dspy.evaluate.evaluate: Average Metric: 26.198653986469825 / 40 (65.5%)\n",
      "2025/06/03 14:27:22 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 65.5 with parameters ['Predictor 0: Instruction 5', 'Predictor 0: Few-Shot Set 4'].\n",
      "2025/06/03 14:27:22 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [57.21, 60.52, 61.11, 60.85, 61.11, 55.55, 60.94, 62.5, 62.12, 59.76, 63.27, 63.27, 61.13, 61.3, 59.99, 60.68, 65.77, 65.77, 65.5]\n",
      "2025/06/03 14:27:22 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 65.77\n",
      "2025/06/03 14:27:22 INFO dspy.teleprompt.mipro_optimizer_v2: =========================\n",
      "\n",
      "\n",
      "2025/06/03 14:27:22 INFO dspy.teleprompt.mipro_optimizer_v2: Returning best identified program with score 65.77!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_18 at: http://localhost:5500/#/experiments/344816129373506955/runs/c6d520b1aeb2449fbc760565d9735561\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 145.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run resilient-shrike-234 at: http://localhost:5500/#/experiments/344816129373506955/runs/a3b3616e10834427b5ac20b2cd63de46\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5500/static-files/lib/notebook-trace-renderer/index.html?trace_id=315875f801ef44308cd7e744ffeadab8&amp;experiment_id=344816129373506955&amp;trace_id=c65eaa3b814641869caf4291aa52ea37&amp;experiment_id=344816129373506955&amp;trace_id=8796a4b478664e2b867cc3f3e06adb17&amp;experiment_id=344816129373506955&amp;trace_id=c2855dc0b1d44b2d85a5c3e483a571be&amp;experiment_id=344816129373506955&amp;trace_id=5b4de4a3edd24b5682fc02a189f36d2e&amp;experiment_id=344816129373506955&amp;trace_id=cc3dccbfcf8f45d1b467d0119aa256a0&amp;experiment_id=344816129373506955&amp;trace_id=dd8ecbcd036041188b4902f9493a5f85&amp;experiment_id=344816129373506955&amp;trace_id=ccecdfbb93f24595acf76060ebfc5363&amp;experiment_id=344816129373506955&amp;trace_id=7bc5e8cafdd14ae8a2691f5de035645b&amp;experiment_id=344816129373506955&amp;trace_id=b797735afa1e49e2a5b55a651e4b8699&amp;experiment_id=344816129373506955&amp;version=2.22.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(request_id=315875f801ef44308cd7e744ffeadab8), Trace(request_id=c65eaa3b814641869caf4291aa52ea37), Trace(request_id=8796a4b478664e2b867cc3f3e06adb17), Trace(request_id=c2855dc0b1d44b2d85a5c3e483a571be), Trace(request_id=5b4de4a3edd24b5682fc02a189f36d2e), Trace(request_id=cc3dccbfcf8f45d1b467d0119aa256a0), Trace(request_id=dd8ecbcd036041188b4902f9493a5f85), Trace(request_id=ccecdfbb93f24595acf76060ebfc5363), Trace(request_id=7bc5e8cafdd14ae8a2691f5de035645b), Trace(request_id=b797735afa1e49e2a5b55a651e4b8699)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tp = dspy.MIPROv2(metric=metric, auto=\"medium\", num_threads=24)  # use fewer threads if your rate limit is small\n",
    "\n",
    "optimized_rag = tp.compile(RAG(), trainset=trainset,\n",
    "                           max_bootstrapped_demos=2, max_labeled_demos=2,\n",
    "                           requires_permission_to_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faa58d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:19:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:19:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:19:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:19:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:19:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:19:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:19:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:19:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use the following methods to switch between hidden or minimized windows on a Mac:\n",
      "- Use `Control-CMD-F` to switch to full screen mode, which will show all open windows.\n",
      "- Navigate to System Preferences > Mission Control, and uncheck \"When switching to an application, switch to a Space with open windows for the application.\"\n",
      "- Alternatively, you can use Automator Services or third-party applications like iCanHazShortcut to execute an `osascript` command that can handle hidden or minimized windows.\n",
      "- Another approach is to use `Cmd+` (backtick) and press `Cmd+tab` while holding `Cmd`, which will show all open applications with animated displayed windows, allowing you to navigate using arrow keys.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5500/static-files/lib/notebook-trace-renderer/index.html?trace_id=3094bf7f5dda413aad0e91acf87c63f0&amp;experiment_id=344816129373506955&amp;version=2.22.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(request_id=3094bf7f5dda413aad0e91acf87c63f0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseline = rag(question=\"cmd+tab does not work on hidden or minimized windows\")\n",
    "print(baseline.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ad0a586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:19:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:20:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:20:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To access minimized windows using cmd+tab, try the following workarounds:\n",
      "- Use Control-CMD-F for full screen mode to access minimized windows.\n",
      "- Switch to all open apps before hiding.\n",
      "- Use the following AppleScript to switch to the first minimized window: `delay 0.5 set i to 0 tell application System Events set first_app to name of the first process whose frontmost is true repeat with p in every process if visible of p then set i to i + 1 end if end repeat repeat i - 1 times key down command key down shift keystroke tab delay 0.01 key up shift key up command delay 0.1 end repeat set visible of process first_app to false end tell\n",
      "- Alternatively, use the following shortcut in iCanHazShortcut: `osascript <scriptname>`\n",
      "- Navigate to the minimized Application by doing Command+Tab while still holding Command, then release both keys.\n",
      "- Press cmd+` to access minimized windows, but first press cmd+tab to open the window, then press cmd+` to minimize it.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5500/static-files/lib/notebook-trace-renderer/index.html?trace_id=2c3b532ff5f549d9bc88381569b506c7&amp;experiment_id=344816129373506955&amp;version=2.22.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(request_id=2c3b532ff5f549d9bc88381569b506c7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = optimized_rag(question=\"cmd+tab does not work on hidden or minimized windows\")\n",
    "print(pred.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cd95b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:20:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:20:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:20:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:20:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:20:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:20:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:28 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:20:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:20:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:30 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:20:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:20:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:20:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:20:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:20:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:20:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:20:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:20:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:20:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:20:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:20:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:20:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:20:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.85 / 1 (84.7%):   1%|          | 1/100 [00:48<1:20:04, 48.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:21:01 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:21:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:21:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:21:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:21:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:21:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:28 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:21:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:44 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:21:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:21:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.51 / 2 (75.7%):   2%|▏         | 2/100 [01:41<1:23:30, 51.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:21:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:21:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:21:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.51 / 3 (50.5%):   3%|▎         | 3/100 [01:44<46:49, 28.97s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:21:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:22:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:22:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:22:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.85 / 4 (46.2%):   4%|▍         | 4/100 [01:52<33:18, 20.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:22:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:22:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:22:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.85 / 5 (36.9%):   5%|▌         | 5/100 [01:57<23:44, 14.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:22:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:22:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:22:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.33 / 6 (38.8%):   6%|▌         | 6/100 [02:02<18:35, 11.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:22:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:22:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:22:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:22:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:22:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.99 / 7 (42.8%):   7%|▋         | 7/100 [02:10<16:15, 10.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:22:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:22:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:22:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.57 / 8 (44.6%):   8%|▊         | 8/100 [02:15<13:23,  8.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:22:28 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:22:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:22:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:22:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:22:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.23 / 9 (47.0%):   9%|▉         | 9/100 [02:25<13:41,  9.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:22:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:22:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:22:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.80 / 10 (48.0%):  10%|█         | 10/100 [02:30<11:43,  7.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:22:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:22:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:22:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.47 / 11 (49.7%):  11%|█         | 11/100 [02:35<10:20,  6.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:22:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:22:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:22:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:22:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.24 / 12 (52.0%):  12%|█▏        | 12/100 [02:39<08:55,  6.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:22:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.93 / 13 (53.3%):  13%|█▎        | 13/100 [02:47<09:32,  6.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:23:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:23:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.93 / 14 (49.5%):  14%|█▍        | 14/100 [02:50<08:11,  5.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:23:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:23:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.53 / 15 (50.2%):  15%|█▌        | 15/100 [02:53<06:48,  4.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:23:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:23:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.30 / 16 (51.9%):  16%|█▌        | 16/100 [02:56<05:56,  4.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:23:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:23:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.97 / 17 (52.8%):  17%|█▋        | 17/100 [03:01<06:07,  4.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:23:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:23:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:23:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:23:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.74 / 18 (54.1%):  18%|█▊        | 18/100 [03:14<09:46,  7.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:23:28 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:23:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:23:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.41 / 19 (54.8%):  19%|█▉        | 19/100 [03:30<13:01,  9.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:23:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:23:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:23:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:23:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:23:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.94 / 20 (54.7%):  20%|██        | 20/100 [03:44<14:35, 10.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:23:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:24:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:24:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:24:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:24:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:24:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:24:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:24:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:24:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:24:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:24:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:24:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:24:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:24:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:24:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:24:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:24:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:24:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:24:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:24:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.73 / 21 (55.9%):  21%|██        | 21/100 [04:36<30:46, 23.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:24:50 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:24:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:24:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.51 / 22 (56.9%):  22%|██▏       | 22/100 [04:41<23:09, 17.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:24:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:24:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:24:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:24:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:25:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.36 / 23 (58.1%):  23%|██▎       | 23/100 [04:51<19:52, 15.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:25:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:25:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.02 / 24 (58.4%):  24%|██▍       | 24/100 [04:52<13:57, 11.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:25:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.69 / 25 (58.8%):  25%|██▌       | 25/100 [04:58<12:08,  9.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:25:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.36 / 26 (59.1%):  26%|██▌       | 26/100 [05:05<10:45,  8.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:25:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.02 / 27 (59.3%):  27%|██▋       | 27/100 [05:08<08:36,  7.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:25:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:25:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.81 / 28 (60.0%):  28%|██▊       | 28/100 [05:15<08:36,  7.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:25:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:25:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:25:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.48 / 29 (60.3%):  29%|██▉       | 29/100 [05:23<08:38,  7.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:25:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:25:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.30 / 30 (61.0%):  30%|███       | 30/100 [05:24<06:12,  5.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:25:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:25:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.07 / 31 (61.5%):  31%|███       | 31/100 [05:28<05:45,  5.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:25:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:25:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.07 / 32 (59.6%):  32%|███▏      | 32/100 [05:30<04:41,  4.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:25:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:25:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.74 / 33 (59.8%):  33%|███▎      | 33/100 [05:33<04:05,  3.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:25:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:25:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.74 / 34 (58.1%):  34%|███▍      | 34/100 [05:38<04:43,  4.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:25:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:25:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:25:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.74 / 35 (56.4%):  35%|███▌      | 35/100 [05:39<03:26,  3.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:25:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:26:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:01 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:26:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.63 / 36 (57.3%):  36%|███▌      | 36/100 [05:51<06:12,  5.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:26:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:26:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:26:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.52 / 37 (58.1%):  37%|███▋      | 37/100 [06:00<07:02,  6.71s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:26:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:26:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:26:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:26:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:26:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:32 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:26:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:26:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.20 / 38 (58.4%):  38%|███▊      | 38/100 [06:28<13:39, 13.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:26:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:26:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 22.87 / 39 (58.6%):  39%|███▉      | 39/100 [06:34<11:21, 11.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:26:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:26:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:26:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:26:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:27:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:27:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:27:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:27:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:27:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:27:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:27:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:27:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:27:20 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:27:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:27:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:27:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:27:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:27:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 23.53 / 40 (58.8%):  40%|████      | 40/100 [07:19<21:16, 21.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:27:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:27:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:27:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:27:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:27:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:27:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:27:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.20 / 41 (59.0%):  41%|████      | 41/100 [07:32<18:18, 18.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:27:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:27:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:27:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24.53 / 42 (58.4%):  42%|████▏     | 42/100 [07:32<12:40, 13.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:27:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:27:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:27:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 25.20 / 43 (58.6%):  43%|████▎     | 43/100 [07:38<10:21, 10.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:27:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:27:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:27:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:27:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 25.80 / 44 (58.6%):  44%|████▍     | 44/100 [07:45<09:03,  9.71s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:27:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:28:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:28:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 25.80 / 45 (57.3%):  45%|████▌     | 45/100 [07:47<06:50,  7.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:28:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:28:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:28:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:28:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:28:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 26.47 / 46 (57.5%):  46%|████▌     | 46/100 [07:55<07:00,  7.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:28:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:28:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:28:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:28:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:28:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 27.07 / 47 (57.6%):  47%|████▋     | 47/100 [08:05<07:27,  8.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:28:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:28:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:28:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 27.67 / 48 (57.6%):  48%|████▊     | 48/100 [08:08<05:48,  6.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:28:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:28:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:28:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 28.51 / 49 (58.2%):  49%|████▉     | 49/100 [08:11<04:48,  5.65s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:28:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:28:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:28:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 29.11 / 50 (58.2%):  50%|█████     | 50/100 [08:16<04:27,  5.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:28:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:28:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:28:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 29.11 / 51 (57.1%):  51%|█████     | 51/100 [08:23<04:41,  5.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:28:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:28:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:28:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 29.78 / 52 (57.3%):  52%|█████▏    | 52/100 [08:24<03:29,  4.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:28:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:28:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:28:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 30.63 / 53 (57.8%):  53%|█████▎    | 53/100 [08:30<03:53,  4.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:28:44 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:28:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:28:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 31.30 / 54 (58.0%):  54%|█████▍    | 54/100 [08:33<03:26,  4.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:28:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:28:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:28:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:50 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:28:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:28:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 31.96 / 55 (58.1%):  55%|█████▌    | 55/100 [08:42<04:12,  5.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:28:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:28:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:28:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:28:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 32.63 / 56 (58.3%):  56%|█████▌    | 56/100 [08:45<03:33,  4.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:28:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:29:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:29:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:29:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:29:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:29:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:29:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:29:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:29:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:29:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:29:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:29:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:29:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 33.30 / 57 (58.4%):  57%|█████▋    | 57/100 [09:16<09:07, 12.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:29:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:29:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:29:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:29:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:29:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 33.91 / 58 (58.5%):  58%|█████▊    | 58/100 [09:27<08:33, 12.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:29:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:29:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:29:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:29:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:29:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:50 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:29:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:29:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:29:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:30:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:30:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:30:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:30:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:30:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:30:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.439768 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.477111 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.440523 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.377408 seconds\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:30:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:30:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:30:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:30:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:30:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.408826 seconds\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:30:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:30:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 34.68 / 59 (58.8%):  59%|█████▉    | 59/100 [10:16<15:49, 23.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:30:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:30:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:30:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:30:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:30:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 35.35 / 60 (58.9%):  60%|██████    | 60/100 [10:22<12:01, 18.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:30:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:30:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:30:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 36.20 / 61 (59.3%):  61%|██████    | 61/100 [10:26<09:08, 14.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:30:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:30:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:30:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 36.20 / 62 (58.4%):  62%|██████▏   | 62/100 [10:28<06:26, 10.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:30:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:30:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:30:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 36.87 / 63 (58.5%):  63%|██████▎   | 63/100 [10:34<05:32,  8.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:30:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:30:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:30:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 37.53 / 64 (58.6%):  64%|██████▍   | 64/100 [10:39<04:38,  7.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:30:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:30:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:30:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:30:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:30:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:30:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 38.31 / 65 (58.9%):  65%|██████▌   | 65/100 [10:46<04:28,  7.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:31:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:31:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:31:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:31:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:31:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 38.92 / 66 (59.0%):  66%|██████▌   | 66/100 [10:55<04:30,  7.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:31:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:31:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:31:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 38.92 / 67 (58.1%):  67%|██████▋   | 67/100 [10:57<03:25,  6.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:31:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:31:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:31:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 39.49 / 68 (58.1%):  68%|██████▊   | 68/100 [11:04<03:27,  6.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:31:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:31:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:31:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 40.09 / 69 (58.1%):  69%|██████▉   | 69/100 [11:05<02:27,  4.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:31:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:31:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 40.76 / 70 (58.2%):  70%|███████   | 70/100 [11:10<02:30,  5.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:31:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:31:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 41.08 / 71 (57.9%):  71%|███████   | 71/100 [11:11<01:47,  3.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:31:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:31:30 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:31:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 41.85 / 72 (58.1%):  72%|███████▏  | 72/100 [11:16<01:58,  4.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:31:30 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:31:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:31:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 42.52 / 73 (58.2%):  73%|███████▎  | 73/100 [11:18<01:28,  3.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:31:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:31:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:31:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:31:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:31:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 43.09 / 74 (58.2%):  74%|███████▍  | 74/100 [11:29<02:32,  5.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:31:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:31:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:31:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 43.76 / 75 (58.3%):  75%|███████▌  | 75/100 [11:34<02:13,  5.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:31:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:31:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:31:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:31:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:32:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:32:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:32:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:32:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:32:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:32:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:32:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:32:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:32:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:32:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 44.51 / 76 (58.6%):  76%|███████▌  | 76/100 [12:05<05:12, 13.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:32:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:32:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:32:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 45.17 / 77 (58.7%):  77%|███████▋  | 77/100 [12:12<04:22, 11.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:32:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:32:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:32:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:32:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:32:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:32:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:32:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:32:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:32:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:32:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:32:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:32:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:32:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:33:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:33:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:01 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:33:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:33:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:33:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:33:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:33:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:33:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 45.77 / 78 (58.7%):  78%|███████▊  | 78/100 [13:02<08:22, 22.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:33:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:33:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:33:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:33:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:33:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:33:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 46.62 / 79 (59.0%):  79%|███████▉  | 79/100 [13:12<06:41, 19.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:33:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:33:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 47.10 / 80 (58.9%):  80%|████████  | 80/100 [13:17<04:59, 14.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:33:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:33:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 47.10 / 81 (58.2%):  81%|████████  | 81/100 [13:19<03:30, 11.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:33:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:33:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 47.77 / 82 (58.3%):  82%|████████▏ | 82/100 [13:25<02:50,  9.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:33:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:33:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 48.44 / 83 (58.4%):  83%|████████▎ | 83/100 [13:31<02:21,  8.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:33:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:33:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:33:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:33:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:33:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 48.97 / 84 (58.3%):  84%|████████▍ | 84/100 [13:41<02:23,  8.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:34:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:34:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 49.58 / 85 (58.3%):  85%|████████▌ | 85/100 [14:10<03:43, 14.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:34:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:34:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:34:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:34:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 50.25 / 86 (58.4%):  86%|████████▌ | 86/100 [14:17<02:56, 12.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:34:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:34:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 51.02 / 87 (58.6%):  87%|████████▋ | 87/100 [14:21<02:08,  9.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:34:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:34:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 51.56 / 88 (58.6%):  88%|████████▊ | 88/100 [14:28<01:48,  9.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:34:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:34:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 51.56 / 89 (57.9%):  89%|████████▉ | 89/100 [14:30<01:17,  7.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:34:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:34:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 52.22 / 90 (58.0%):  90%|█████████ | 90/100 [14:34<00:59,  5.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:34:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:34:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 53.08 / 91 (58.3%):  91%|█████████ | 91/100 [14:39<00:51,  5.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:34:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:34:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 53.62 / 92 (58.3%):  92%|█████████▏| 92/100 [14:45<00:45,  5.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:34:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:34:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:34:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 54.22 / 93 (58.3%):  93%|█████████▎| 93/100 [14:45<00:29,  4.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:35:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:35:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:35:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:35:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 54.99 / 94 (58.5%):  94%|█████████▍| 94/100 [14:55<00:34,  5.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:35:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:35:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:35:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:35:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:35:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:35:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 55.66 / 95 (58.6%):  95%|█████████▌| 95/100 [15:14<00:48,  9.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:35:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:35:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 56.23 / 96 (58.6%):  96%|█████████▌| 96/100 [15:19<00:33,  8.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:35:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:35:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 56.83 / 97 (58.6%):  97%|█████████▋| 97/100 [15:23<00:20,  6.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:35:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:35:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m15:35:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:35:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:35:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 57.60 / 98 (58.8%):  98%|█████████▊| 98/100 [15:26<00:11,  5.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:35:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:35:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 58.46 / 99 (59.0%):  99%|█████████▉| 99/100 [15:30<00:05,  5.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:35:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:35:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 59.31 / 100 (59.3%): 100%|██████████| 100/100 [15:32<00:00,  9.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 15:35:45 INFO dspy.evaluate.evaluate: Average Metric: 59.30611045825858 / 100 (59.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>example_response</th>\n",
       "      <th>gold_doc_ids</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>pred_response</th>\n",
       "      <th>SemanticF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>does using == in javascript ever make sense?</td>\n",
       "      <td>Yes, using `==` in JavaScript can make sense and is convenient in ...</td>\n",
       "      <td>[5778, 5791, 5818]</td>\n",
       "      <td>The use of `==` in JavaScript can be misleading due to type coerci...</td>\n",
       "      <td>Yes, using `==` in JavaScript can make sense in certain situations...</td>\n",
       "      <td>✔️ [0.667]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is the difference between a virus and trojan?</td>\n",
       "      <td>The terms have a great deal of overlap and aren't necessarily mutu...</td>\n",
       "      <td>[3768, 3769, 3888, 3890, 4046]</td>\n",
       "      <td>A virus and a Trojan are both types of malware, but they differ in...</td>\n",
       "      <td>A virus and a Trojan are different types of malware. A virus is a ...</td>\n",
       "      <td>✔️ [0.774]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0        does using == in javascript ever make sense?   \n",
       "1  what is the difference between a virus and trojan?   \n",
       "\n",
       "                                                        example_response  \\\n",
       "0  Yes, using `==` in JavaScript can make sense and is convenient in ...   \n",
       "1  The terms have a great deal of overlap and aren't necessarily mutu...   \n",
       "\n",
       "                     gold_doc_ids  \\\n",
       "0              [5778, 5791, 5818]   \n",
       "1  [3768, 3769, 3888, 3890, 4046]   \n",
       "\n",
       "                                                               reasoning  \\\n",
       "0  The use of `==` in JavaScript can be misleading due to type coerci...   \n",
       "1  A virus and a Trojan are both types of malware, but they differ in...   \n",
       "\n",
       "                                                           pred_response  \\\n",
       "0  Yes, using `==` in JavaScript can make sense in certain situations...   \n",
       "1  A virus and a Trojan are different types of malware. A virus is a ...   \n",
       "\n",
       "   SemanticF1  \n",
       "0  ✔️ [0.667]  \n",
       "1  ✔️ [0.774]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div style='\n",
       "                text-align: center;\n",
       "                font-size: 16px;\n",
       "                font-weight: bold;\n",
       "                color: #555;\n",
       "                margin: 10px 0;'>\n",
       "                ... 98 more rows not displayed ...\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run eval at: http://localhost:5500/#/experiments/344816129373506955/runs/b07c1bab32b4464e83ab35e35a59514f\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/344816129373506955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59.31"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5500/static-files/lib/notebook-trace-renderer/index.html?trace_id=6ce87b31a5a9490787082a0c72466ff4&amp;experiment_id=344816129373506955&amp;trace_id=2ff861482efc49f5a0e7e461c665da97&amp;experiment_id=344816129373506955&amp;trace_id=601b669e00114e56a7caf467410f5ebc&amp;experiment_id=344816129373506955&amp;trace_id=23616567361a4aae83b56df1642fec61&amp;experiment_id=344816129373506955&amp;trace_id=8c1fe220a33949b5bfe961ea34674fc9&amp;experiment_id=344816129373506955&amp;trace_id=18d003a934a64cd8971c4d7f098d9848&amp;experiment_id=344816129373506955&amp;trace_id=41ab560ff8f64bf79828013db77ebef1&amp;experiment_id=344816129373506955&amp;trace_id=b0393c437fc3440f9ff9c06967301b7d&amp;experiment_id=344816129373506955&amp;trace_id=c2c4c2e0e10f40f7a49621561603c3fd&amp;experiment_id=344816129373506955&amp;trace_id=104e3261472a40df86a7df3743b1b805&amp;experiment_id=344816129373506955&amp;version=2.22.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(request_id=6ce87b31a5a9490787082a0c72466ff4), Trace(request_id=2ff861482efc49f5a0e7e461c665da97), Trace(request_id=601b669e00114e56a7caf467410f5ebc), Trace(request_id=23616567361a4aae83b56df1642fec61), Trace(request_id=8c1fe220a33949b5bfe961ea34674fc9), Trace(request_id=18d003a934a64cd8971c4d7f098d9848), Trace(request_id=41ab560ff8f64bf79828013db77ebef1), Trace(request_id=b0393c437fc3440f9ff9c06967301b7d), Trace(request_id=c2c4c2e0e10f40f7a49621561603c3fd), Trace(request_id=104e3261472a40df86a7df3743b1b805)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:35:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:35:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:35:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:35:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:35:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:35:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    }
   ],
   "source": [
    "evaluate(optimized_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03a83d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning='The issue with cmd+tab not working on hidden or minimized windows is due to a change in the behavior of the Mission Control system preference. The default setting now hides tabs when switching to an application, making it difficult to access minimized windows using cmd+tab. Various workarounds have been suggested, including using Control-CMD-F for full screen mode, switching to all open apps before hiding, or using third-party applications like iCanHazShortcut or Automator Services.',\n",
       "    response='To access minimized windows using cmd+tab, try the following workarounds:\\n- Use Control-CMD-F for full screen mode to access minimized windows.\\n- Switch to all open apps before hiding.\\n- Use the following AppleScript to switch to the first minimized window: `delay 0.5 set i to 0 tell application System Events set first_app to name of the first process whose frontmost is true repeat with p in every process if visible of p then set i to i + 1 end if end repeat repeat i - 1 times key down command key down shift keystroke tab delay 0.01 key up shift key up command delay 0.1 end repeat set visible of process first_app to false end tell\\n- Alternatively, use the following shortcut in iCanHazShortcut: `osascript <scriptname>`\\n- Navigate to the minimized Application by doing Command+Tab while still holding Command, then release both keys.\\n- Press cmd+` to access minimized windows, but first press cmd+tab to open the window, then press cmd+` to minimize it.'\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5500/static-files/lib/notebook-trace-renderer/index.html?trace_id=b7a0a8d5c5054c2b94793d42929fb6ce&amp;experiment_id=344816129373506955&amp;version=2.22.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(request_id=b7a0a8d5c5054c2b94793d42929fb6ce)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:35:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m15:35:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    }
   ],
   "source": [
    "optimized_rag.save(\"optimized_rag.json\")\n",
    "\n",
    "loaded_rag = RAG()\n",
    "loaded_rag.load(\"optimized_rag.json\")\n",
    "\n",
    "loaded_rag(question=\"cmd+tab does not work on hidden or minimized windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c72cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
