{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e700782",
   "metadata": {},
   "source": [
    "[MIPROv2](https://dspy.ai/api/optimizers/MIPROv2/)\n",
    "\n",
    "At a high level, MIPROv2 works by creating both few-shot examples and new instructions for each predictor in your LM program, and then searching over these using Bayesian Optimization to find the best combination of these variables for your program. If you want a visual explanation check out this [twitter thread](https://x.com/michaelryan207/status/1804189184988713065)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8740fcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/git/dspy-tool-use/venv/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5500\")\n",
    "mlflow.set_experiment(\"optimize-react\")\n",
    "mlflow.dspy.autolog(\n",
    "    log_compiles=True,    # Track optimization process\n",
    "    log_evals=True,       # Track evaluation results\n",
    "    log_traces_from_compile=True  # Track program traces during optimization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8dded91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import openai\n",
    "import os\n",
    "\n",
    "LLM_URL=os.getenv('LLM_URL', 'http://localhost:8080/v1')\n",
    "API_KEY=os.getenv('API_KEY', 'fake')\n",
    "LLM_MODEL=os.getenv('LLM_MODEL', 'openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf')\n",
    "MAX_TOKENS=os.getenv('MAX_TOKENS', 3000)\n",
    "TEMPERATURE=os.getenv('TEMPERATURE', 0.2)\n",
    "dspy.enable_logging()\n",
    "lm = dspy.LM(model=LLM_MODEL,\n",
    "             api_base=LLM_URL,  # ensure this points to your port\n",
    "             api_key=API_KEY,\n",
    "             temperature=TEMPERATURE,\n",
    "             model_type='chat',\n",
    "             stream=False)\n",
    "dspy.configure(lm=lm)\n",
    "dspy.settings.configure(track_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85202e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/mike/.cache/huggingface/modules/datasets_modules/datasets/hotpot_qa/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5 (last modified on Sat Aug 17 10:45:14 2024) since it couldn't be found locally at hotpot_qa, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from /home/mike/.cache/huggingface/modules/datasets_modules/datasets/hotpot_qa/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5 (last modified on Sat Aug 17 10:45:14 2024) since it couldn't be found locally at hotpot_qa, or remotely on the Hugging Face Hub.\n",
      "2025/06/03 10:06:19 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '6308fe4da04040d1821126afa9d02c37', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current dspy workflow\n",
      "2025/06/03 10:06:19 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "2025/06/03 10:06:19 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/06/03 10:06:19 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=10 sets of demonstrations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 1/10\n",
      "Bootstrapping set 2/10\n",
      "Bootstrapping set 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 160.32it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 16.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples for up to 1 rounds, amounting to 4 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 178.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 195.28it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 20.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples for up to 1 rounds, amounting to 4 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 208.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 255.10it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 20.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples for up to 1 rounds, amounting to 4 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 231.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 247.54it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 18.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples for up to 1 rounds, amounting to 4 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 200.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 335.36it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples for up to 1 rounds, amounting to 4 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 225.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 214.88it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 20.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples for up to 1 rounds, amounting to 4 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 218.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 256.60it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 21.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples for up to 1 rounds, amounting to 4 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 239.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 206.32it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 19.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples for up to 1 rounds, amounting to 4 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 269.37it/s]\n",
      "2025/06/03 10:06:21 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "2025/06/03 10:06:21 INFO dspy.teleprompt.mipro_optimizer_v2: We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n",
      "2025/06/03 10:06:22 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "Proposing N=10 instructions...\n",
      "\n",
      "\u001b[92m10:06:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:06:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:06:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:06:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:06:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:06:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:06:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:06:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:07:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:07:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:07:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:07:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:07:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:07:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:07:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:07:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:07:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:07:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:07:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:08:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:08:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:01 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:08:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:08:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:08:13 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:08:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:08:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:08:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:08:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:08:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:08:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:08:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:08:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:09:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:09:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:09:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:09:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:09:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:09:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:09:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:09:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:10:32 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:10:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:10:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:10:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:10:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:10:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:10:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:10:32 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:10:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:10:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:10:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:10:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:10:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:10:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:10:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:10:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:11:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:11:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:11:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:11:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:11:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:11:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:11:16 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:12:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:12:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:12:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:12:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:12:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:12:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:12:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 0:\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 0: Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "You are an Agent. In each episode, you will be given the fields `question` as input. And you can see your past trajectory so far.\n",
      "Your goal is to use one or more of the supplied tools to collect any necessary information for producing `answer`.\n",
      "\n",
      "To do this, you will interleave next_thought, next_tool_name, and next_tool_args in each turn, and also when finishing the task.\n",
      "After each tool call, you receive a resulting observation, which gets appended to your trajectory.\n",
      "\n",
      "When writing next_thought, you may reason about the current situation and plan for future steps.\n",
      "When selecting the next_tool_name and its next_tool_args, the tool must be one of:\n",
      "\n",
      "(1) search, whose description is <desc>Retrieves abstracts from Wikipedia.</desc>. It takes arguments {'query': {'type': 'string'}} in JSON format.\n",
      "(2) finish, whose description is <desc>Marks the task as complete. That is, signals that all information for producing the outputs, i.e. `answer`, are now available to be extracted.</desc>. It takes arguments {} in JSON format.\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 1: You are an Agent. In each episode, you will be given the fields `question` as input. And you can see your past trajectory so far. Your goal is to use one or more of the supplied tools to collect any necessary information for producing `answer`. To do this, you will interleave next_thought, next_tool_name, and next_tool_args in each turn, and also when finishing the task. After each tool call, you receive a resulting observation, which gets appended to your trajectory. When writing next_thought, you may reason about the current situation and plan for future steps. When selecting the next_tool_name and its next_tool_args, the tool must be one of: (1) search, whose description is Retrieves abstracts from Wikipedia. It takes arguments {'query': {'type': 'string'}} in JSON format. (2) finish, whose description is Marks the task as complete. That is, signals that all information for producing the outputs, i.e. answer, are now available to be extracted. It takes arguments {} in JSON format. Use the search tool with a query that is a string, and then use the finish tool to mark the task as complete when you have the answer.\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 2: You are an Agent. In each episode, you will be given the fields `question` as input. And you can see your past trajectory so far. Your goal is to use one or more of the supplied tools to collect any necessary information for producing `answer`. To do this, you will interleave `next_thought`, `next_tool_name`, and `next_tool_args` in each turn, and also when finishing the task. After each tool call, you receive a resulting observation, which gets appended to your trajectory. When writing `next_thought`, you may reason about the current situation and plan for future steps. When selecting the `next_tool_name` and its `next_tool_args`, ensure that the `next_tool_name` is either \"search\" or \"finish\", and that the `next_tool_args` for \"search\" is a JSON-formatted string query. When the `next_tool_name` is \"finish\", the `next_tool_args` should be an empty dictionary. When the `next_tool_name` is \"search\", the `next_tool_args` should be a dictionary with a single key-value pair, where the key is \"query\" and the value is a string query. Next, use the \"search\" tool to retrieve abstracts from Wikipedia, passing in the query as a JSON-formatted string. Finally, use the \"finish\" tool to mark the task as complete, passing in an empty dictionary as the tool arguments.\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 3: You are an Agent. In each episode, you will be given the fields `question` as input. And you can see your past trajectory so far. Your goal is to use one or more of the supplied tools to collect any necessary information for producing `answer`. To do this, you will interleave next_thought, next_tool_name, and next_tool_args in each turn, interleaving next_thought, next_tool_name, and next_tool_args in each turn, and also when finishing the task. After each tool call, you receive a resulting observation, which gets appended to your trajectory. When writing next_thought, you may reason about the current situation and plan for future steps. When selecting the next_tool_name and its next_tool_args, the tool must be one of: (1) search, whose description is Retrieves abstracts from Wikipedia. It takes arguments {'query': {'type': 'string'}} in JSON format. (2) finish, whose description is Marks the task as complete. That is, signals that all information for producing the outputs, i.e. answer, are now available to be extracted. It takes arguments {} in JSON format.\n",
      "\n",
      "You are an Agent. In each episode, you will be given the fields `question` as input. And you can see your past trajectory so far. Your goal is to use one or more of the supplied tools to collect any necessary information for producing `answer`. To do this, you will interleave next_thought, next_tool_name, and next_tool_args in each turn, interleaving next_thought, next_tool_name, and next_tool_args in each turn, and also when finishing the task. After each tool call, you receive a resulting observation, which gets appended to your trajectory. When writing next_thought, you may reason about the current situation and plan for future steps. When selecting the next_tool_name and its next_tool_args, the tool must be one of: (1) search, whose description is Retrieves abstracts from Wikipedia. It takes arguments {'query': {'type': 'string'}} in JSON format. (2) finish, whose description is Marks the task as complete. That is, signals that all information for producing the outputs, i.e. answer, are now available to be extracted. It takes arguments {} in JSON format.\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 4: Given the fields `question`, produce the fields `answer`. You are an Agent. In each episode, you will be given the fields `question` as input. And you can see your past trajectory so far. Your goal is to use one or more of the supplied tools to collect any necessary information for producing `answer`. To do this, use the `search` tool to retrieve information related to the `question` by passing a string query, and then use the `finish` tool to signal that the task is complete. When writing `next_thought`, you may reason about the current situation and plan for future steps. When selecting the `next_tool_name` and its `next_tool_args`, the tool must be one of `search`, whose description is Retrieves abstracts from Wikipedia. It takes arguments `{'query': {'type': 'string'}}` in JSON format, or `finish`, whose description is Marks the task as complete. That is, signals that all information for producing the outputs, i.e. `answer`, are now available to be extracted. It takes arguments `{}` in JSON format.\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 5: You are an Agent. In each episode, you will be given the fields `question` as input. And you can see your past trajectory so far. Your goal is to use one or more of the supplied tools to collect any necessary information for producing `answer`. To do this, you will interleave next_thought, next_tool_name, and next_tool_args in each turn, and also when finishing the task. After each tool call, you receive a resulting observation, which gets appended to your trajectory. When writing next_thought, you may reason about the current situation and plan for future steps. When selecting the next_tool_name and its next_tool_args, the tool must be one of: (1) search, whose description is Retrieves abstracts from Wikipedia. It takes arguments {'query': {'type': 'string'}} in JSON format. (2) finish, whose description is Marks the task as complete. That is, signals that all information for producing the outputs, i.e. `answer`, are now available to be extracted. It takes arguments {} in JSON format. Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 6: Given the fields `question`, use the search tool to retrieve abstracts from Wikipedia with the query containing the question, and then use the finish tool to mark the task as complete once the answer is produced.\n",
      "\n",
      "You are an Agent. In each episode, you will be given the fields `question` as input. And you can see your past trajectory so far.\n",
      "Your goal is to use one or more of the supplied tools to collect any necessary information for producing `answer`.\n",
      "\n",
      "To do this, you will interleave next_thought, next_tool_name, and next_tool_args in each turn, and also when finishing the task.\n",
      "After each tool call, you receive a resulting observation, which gets appended to your trajectory.\n",
      "\n",
      "When writing next_thought, you may reason about the current situation and plan for future steps.\n",
      "When selecting the next_tool_name and its next_tool_args, the tool must be one of:\n",
      "\n",
      "(1) search, whose description is <desc>Retrieves abstracts from Wikipedia.</desc>. It takes arguments {'query': {'type': 'string'}} in JSON format.\n",
      "(2) finish, whose description is <desc>Marks the task as complete. That is, signals that all information for producing the outputs, i.e. `answer`, are now available to be extracted.</desc>. It takes arguments {} in JSON format.\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 7: You are an Agent. In each episode, you will be given the fields `question` as input. And you can see your past trajectory so far. Your goal is to use one or more of the supplied tools to collect any necessary information for producing `answer`. To do this, you will interleave reasoning steps and tool calls in each turn, interleaving your next thought with the tool name and its arguments, and also when finishing the task. After each tool call, you receive a resulting observation, which gets appended to your trajectory. When writing next thought, you may reason about the current situation and plan for future steps. When selecting the next tool name and its tool arguments, the tool must be one of: (1) search, whose description is Retrieves abstracts from Wikipedia. It takes arguments {'query': {'type': 'string'}} in JSON format. (2) finish, whose description is Marks the task as complete. That is, signals that all information for producing the outputs, i.e. answer, are now available to be extracted. It takes arguments {} in JSON format.\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 8: You are an Agent tasked with answering a question by utilizing the tools available. You will be given a question as input, and your goal is to use one or more tools to collect necessary information for producing an answer. The tools at your disposal are \"search\" and \"finish.\" The \"search\" tool retrieves abstracts from Wikipedia and takes a query in JSON format, while the \"finish\" tool marks the task as complete and signals that all necessary information for producing the output is available. You will interleave your next thought, tool name, and tool arguments in each turn, and append the resulting observation to your trajectory. When writing your next thought, you may reason about the current situation and plan for future steps. You must select a tool name and its tool arguments from the available options. The question is: That Darn Cat! and Never a Dull Moment were both produced by what studio?\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 9: Given the fields `question`, use the `search` tool to retrieve relevant information from Wikipedia, and then use the `finish` tool to mark the task as complete, producing the field `answer` as output.\n",
      "\n",
      "You are an Agent. In each episode, you will be given the fields `question` as input. And you can see your past trajectory so far.\n",
      "Your goal is to use one or more of the supplied tools to collect any necessary information for producing `answer`.\n",
      "\n",
      "To do this, you will interleave next_thought, next_tool_name, and next_tool_args in each turn, and also when finishing the task.\n",
      "After each tool call, you receive a resulting observation, which gets appended to your trajectory.\n",
      "\n",
      "When writing next_thought, you may reason about the current situation and plan for future steps.\n",
      "When selecting the next_tool_name and its next_tool_args, the tool must be one of:\n",
      "\n",
      "(1) search, whose description is Retrieves abstracts from Wikipedia. It takes arguments {'query': {'type': 'string'}} in JSON format.\n",
      "(2) finish, whose description is Marks the task as complete. That is, signals that all information for producing the outputs, i.e. answer, are now available to be extracted. It takes arguments {} in JSON format.\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 1:\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 0: Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 1: \n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 2: Walt Disney Productions\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 3: Given the fields `query` and `studio`, produce the field `[[ ## answer ## ]]` containing the answer to the question \"That Darn Cat! and Never a Dull Moment were both produced by what studio?\".\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 4: Walt Disney Productions\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 5: Walt Disney Productions\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 6: The studio behind That Darn Cat! and Never a Dull Moment is Disney.\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 7: In a high-stakes movie trivia game, you're asked to identify the studio behind the films \"That Darn Cat!\" and \"Never a Dull Moment\". The player is given a dictionary with a 'query' key, but it's not a string, but rather a dictionary with a 'type' key. The player must correct the query to provide a valid string.\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 8: The studio behind That Darn Cat! and Never a Dull Moment is Disney.\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: 9: The proposed instruction is to analyze the error message and provide the correct answer.\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: ==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
      "\n",
      "2025/06/03 10:14:33 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 1 / 4 - Full Evaluation of Default Program ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 16 (0.0%): 100%|██████████| 16/16 [00:01<00:00,  9.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 10:14:35 INFO dspy.evaluate.evaluate: Average Metric: 0 / 16 (0.0%)\n",
      "2025/06/03 10:14:35 INFO dspy.teleprompt.mipro_optimizer_v2: Default program score: 0.0\n",
      "\n",
      "/home/mike/git/dspy-tool-use/venv/lib64/python3.12/site-packages/optuna/_experimental.py:31: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "2025/06/03 10:14:35 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 2 / 4 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_0 at: http://localhost:5500/#/experiments/302620010842219766/runs/513704c4a6bb4d33b9ddb53d0263eb18\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/302620010842219766\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:14:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:14:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:14:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:14:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:14:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:14:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:14:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:14:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:14:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:14:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:44 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:50 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:14:50 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:14:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:14:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:14:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:26 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:40 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:40 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:43 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:46 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:46 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:46 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:52 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:15:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:15:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:15:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:16:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:16:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:16:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:16:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:16:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:16:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:16:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:16:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:16:18 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:16:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:18 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:16:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:16:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:16:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:16:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 1 (0.0%):  10%|█         | 1/10 [01:53<17:00, 113.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:16:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:16:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 2 (0.0%):  20%|██        | 2/10 [01:58<06:38, 49.82s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:16:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:16:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.00 / 3 (33.3%):  30%|███       | 3/10 [02:02<03:22, 28.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:16:42 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:16:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:16:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:16:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.00 / 4 (25.0%):  40%|████      | 4/10 [02:16<02:17, 22.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:16:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:16:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:16:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.00 / 5 (20.0%):  50%|█████     | 5/10 [02:22<01:24, 16.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.00 / 6 (16.7%):  60%|██████    | 6/10 [02:27<00:50, 12.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.00 / 7 (14.3%):  70%|███████   | 7/10 [02:49<00:47, 15.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:17:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:17:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:17:24 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.00 / 8 (12.5%):  80%|████████  | 8/10 [02:54<00:25, 12.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.00 / 9 (11.1%):  90%|█████████ | 9/10 [03:01<00:10, 10.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.00 / 10 (10.0%): 100%|██████████| 10/10 [03:06<00:00, 18.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 10:17:41 INFO dspy.evaluate.evaluate: Average Metric: 1 / 10 (10.0%)\n",
      "2025/06/03 10:17:41 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 10.0 on minibatch of size 10 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 2', 'Predictor 1: Instruction 6', 'Predictor 1: Few-Shot Set 2'].\n",
      "2025/06/03 10:17:41 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [10.0]\n",
      "2025/06/03 10:17:41 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [0.0]\n",
      "2025/06/03 10:17:41 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 0.0\n",
      "2025/06/03 10:17:41 INFO dspy.teleprompt.mipro_optimizer_v2: ========================================\n",
      "\n",
      "\n",
      "2025/06/03 10:17:41 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 3 / 4 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_minibatch_0 at: http://localhost:5500/#/experiments/302620010842219766/runs/642f1a17ee814a7291abe675d5b27150\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/302620010842219766\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:17:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:17:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:17:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:17:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:17:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:17:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:17:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:17:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:17:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:17:42 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:49 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:53 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:53 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:56 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:17:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:17:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:17:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:17:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:18:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:04 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:11 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:13 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:16 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:19 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:19 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:20 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:23 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:23 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:23 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:29 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:32 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:34 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:34 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:39 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:39 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:41 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:41 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:48 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:54 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:54 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:54 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:18:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:18:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:19:02 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:19:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:02 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:19:06 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:19:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:06 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:06 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:19:09 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:19:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:09 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:09 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:19:11 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:19:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 1 (0.0%):  10%|█         | 1/10 [01:29<13:25, 89.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:19:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:19:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:14 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:19:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:19:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 2 (0.0%):  20%|██        | 2/10 [01:40<05:48, 43.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:19:27 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:19:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:27 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:19:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:19:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:19:35 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:19:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:35 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:19:38 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:19:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:38 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:38 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:19:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:19:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:45 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:19:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:19:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 3 (0.0%):  30%|███       | 3/10 [02:10<04:21, 37.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:19:56 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:19:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:56 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 4 (0.0%):  40%|████      | 4/10 [02:14<02:23, 23.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:19:59 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:19:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:59 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:19:59 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:20:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.00 / 5 (20.0%):  50%|█████     | 5/10 [02:28<01:42, 20.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:20:14 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 6 (33.3%):  60%|██████    | 6/10 [02:32<00:59, 14.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:20:20 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 7 (28.6%):  70%|███████   | 7/10 [02:38<00:36, 12.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:20:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:20:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:20:20 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:20:26 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:26 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 8 (25.0%):  80%|████████  | 8/10 [02:44<00:20, 10.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:20:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 9 (22.2%):  90%|█████████ | 9/10 [02:49<00:08,  8.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:20:36 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:36 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 10 (20.0%): 100%|██████████| 10/10 [02:54<00:00, 17.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 10:20:36 INFO dspy.evaluate.evaluate: Average Metric: 2 / 10 (20.0%)\n",
      "2025/06/03 10:20:36 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 20.0 on minibatch of size 10 with parameters ['Predictor 0: Instruction 8', 'Predictor 0: Few-Shot Set 6', 'Predictor 1: Instruction 4', 'Predictor 1: Few-Shot Set 5'].\n",
      "2025/06/03 10:20:36 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [10.0, 20.0]\n",
      "2025/06/03 10:20:36 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [0.0]\n",
      "2025/06/03 10:20:36 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 0.0\n",
      "2025/06/03 10:20:36 INFO dspy.teleprompt.mipro_optimizer_v2: ========================================\n",
      "\n",
      "\n",
      "2025/06/03 10:20:36 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 4 / 4 - Full Evaluation =====\n",
      "2025/06/03 10:20:36 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 20.0) from minibatch trials...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_minibatch_1 at: http://localhost:5500/#/experiments/302620010842219766/runs/c02400ecdd5a4406a0171c798caaa78b\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/302620010842219766\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:20:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:20:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:20:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:20:36 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:20:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "\u001b[92m10:20:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 10 (20.0%):  56%|█████▋    | 9/16 [00:01<00:00, 14.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:20:43 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:20:45 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:20:48 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:48 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 10 (20.0%):  62%|██████▎   | 10/16 [00:12<00:00, 14.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:20:50 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:50 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:20:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:53 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:20:53 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:53 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:53 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:20:55 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:55 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:55 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:20:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:20:58 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:58 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:20:58 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:00 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:00 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:01 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:01 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:04 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:05 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:05 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:07 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:07 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:08 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:08 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:10 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:10 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:12 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:17 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:17 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:17 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:21 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:21 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:22 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:22 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:25 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:25 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:28 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:28 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:31 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:33 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:33 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:37 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:37 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:44 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:44 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:47 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:52 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:52 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 11 (18.2%):  69%|██████▉   | 11/16 [01:15<00:53, 10.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:21:57 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:21:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:57 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:21:57 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:22:03 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:03 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:03 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:22:12 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:22:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:12 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 12 (16.7%):  75%|███████▌  | 12/16 [01:35<00:48, 12.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:22:15 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:22:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:15 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= models/Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:22:24 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:22:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 13 (15.4%):  81%|████████▏ | 13/16 [01:47<00:36, 12.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:22:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:22:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.00 / 14 (14.3%):  88%|████████▊ | 14/16 [01:52<00:21, 10.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:22:29 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:22:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.00 / 15 (20.0%):  88%|████████▊ | 14/16 [01:53<00:21, 10.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:22:31 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:22:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:22:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: models/Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.00 / 16 (18.8%): 100%|██████████| 16/16 [01:54<00:00,  7.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 10:22:31 INFO dspy.evaluate.evaluate: Average Metric: 3 / 16 (18.8%)\n",
      "2025/06/03 10:22:31 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mNew best full eval score!\u001b[0m Score: 18.75\n",
      "2025/06/03 10:22:31 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [0.0, 18.75]\n",
      "2025/06/03 10:22:31 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 18.75\n",
      "2025/06/03 10:22:31 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "2025/06/03 10:22:31 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/06/03 10:22:31 INFO dspy.teleprompt.mipro_optimizer_v2: Returning best identified program with score 18.75!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_full_1 at: http://localhost:5500/#/experiments/302620010842219766/runs/8a401584b82a4938a9ae34281dc69924\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/302620010842219766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 149.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run delightful-stoat-295 at: http://localhost:5500/#/experiments/302620010842219766/runs/6308fe4da04040d1821126afa9d02c37\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/302620010842219766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5500/static-files/lib/notebook-trace-renderer/index.html?trace_id=ebf6c00c07334097861866a7fe14584c&amp;experiment_id=302620010842219766&amp;trace_id=97a56318c9094a7c9a342dd9af2602b6&amp;experiment_id=302620010842219766&amp;trace_id=d1304a8ce5f54e77a67c470c8b59080d&amp;experiment_id=302620010842219766&amp;trace_id=8071a6349d0f478495c21fcacf9ce07e&amp;experiment_id=302620010842219766&amp;trace_id=c020be3512484a84a9df60520b2b73d8&amp;experiment_id=302620010842219766&amp;trace_id=a5715b8b4f9d4a60852d99c869285e43&amp;experiment_id=302620010842219766&amp;trace_id=d8e8f2f792224b5ab015d82483cdfb43&amp;experiment_id=302620010842219766&amp;trace_id=3087a280626043fdba8cb3cc49fb5529&amp;experiment_id=302620010842219766&amp;trace_id=f7c5240269474f93a655a3c35a6faa3b&amp;experiment_id=302620010842219766&amp;trace_id=3cdf50c110f54b0d92c4fa950fac974f&amp;experiment_id=302620010842219766&amp;version=2.22.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(request_id=ebf6c00c07334097861866a7fe14584c), Trace(request_id=97a56318c9094a7c9a342dd9af2602b6), Trace(request_id=d1304a8ce5f54e77a67c470c8b59080d), Trace(request_id=8071a6349d0f478495c21fcacf9ce07e), Trace(request_id=c020be3512484a84a9df60520b2b73d8), Trace(request_id=a5715b8b4f9d4a60852d99c869285e43), Trace(request_id=d8e8f2f792224b5ab015d82483cdfb43), Trace(request_id=3087a280626043fdba8cb3cc49fb5529), Trace(request_id=f7c5240269474f93a655a3c35a6faa3b), Trace(request_id=3cdf50c110f54b0d92c4fa950fac974f)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dspy.datasets import HotPotQA\n",
    "\n",
    "def search(query: str) -> list[str]:\n",
    "    \"\"\"Retrieves abstracts from Wikipedia.\"\"\"\n",
    "    results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3)\n",
    "    return [x['text'] for x in results]\n",
    "\n",
    "trainset = [x.with_inputs('question') for x in HotPotQA(train_seed=2024, train_size=20).train]\n",
    "react = dspy.ReAct(\"question -> answer\", tools=[search])\n",
    "\n",
    "tp = dspy.MIPROv2(metric=dspy.evaluate.answer_exact_match, auto=None, num_threads=24, num_candidates=10)  # auto=\"light\"\n",
    "optimized_react = tp.compile(react, trainset=trainset, requires_permission_to_run=False, num_trials=2, minibatch=True, minibatch_size=10, minibatch_full_eval_steps=2)\n",
    "\n",
    "optimized_react.save(path=\"optimized_react.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "605de5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    trajectory={'thought_0': 'Liverpool is located in the north of England.', 'tool_name_0': 'search', 'tool_args_0': {'query': 'Is Liverpool in northern England?'}, 'observation_0': ['Liverpool | Liverpool ( ) is a city in North West England, with an estimated population of 478,580 in 2015. With its surrounding areas, it is the fifth-largest metropolitan area in the UK, with over 2.24 million people in 2011. The local authority is Liverpool City Council, the most populous local government district within the metropolitan county of Merseyside and the largest within the Liverpool City Region.', 'Demography of Liverpool | The demography of Liverpool is officially analysed by the Office for National Statistics. The City of Liverpool together with the Metropolitan Borough of Sefton, the Metropolitan Borough of Knowsley, the Metropolitan Borough of St Helens and the Metropolitan Borough of Wirral forms the metropolitan county of Merseyside. With a population of around 466,400, Liverpool is the largest settlement in the county, the largest in North West England, and the fourth largest in the United Kingdom.', 'Leeds and Liverpool Canal | The Leeds and Liverpool Canal is a canal in Northern England, linking the cities of Leeds and Liverpool.'], 'thought_1': 'The search results confirm that Liverpool is located in the north of England.', 'tool_name_1': 'search', 'tool_args_1': {'query': 'Liverpool location in England'}, 'observation_1': ['Liverpool | Liverpool ( ) is a city in North West England, with an estimated population of 478,580 in 2015. With its surrounding areas, it is the fifth-largest metropolitan area in the UK, with over 2.24 million people in 2011. The local authority is Liverpool City Council, the most populous local government district within the metropolitan county of Merseyside and the largest within the Liverpool City Region.', 'Liverpool (disambiguation) | Liverpool is a city in Merseyside, England.', 'Liverpool, London and Globe Building | The Liverpool, London and Globe Building is located in Dale Street, Liverpool, Merseyside, England. It fills a block adjacent to the Town Hall, bounded to the northeast by Exchange Street East and to the southwest by High Street.'], 'thought_2': 'The search results confirm that Liverpool is located in the north of England.', 'tool_name_2': 'search', 'tool_args_2': {'query': 'Liverpool location in England'}, 'observation_2': ['Liverpool | Liverpool ( ) is a city in North West England, with an estimated population of 478,580 in 2015. With its surrounding areas, it is the fifth-largest metropolitan area in the UK, with over 2.24 million people in 2011. The local authority is Liverpool City Council, the most populous local government district within the metropolitan county of Merseyside and the largest within the Liverpool City Region.', 'Liverpool (disambiguation) | Liverpool is a city in Merseyside, England.', 'Liverpool, London and Globe Building | The Liverpool, London and Globe Building is located in Dale Street, Liverpool, Merseyside, England. It fills a block adjacent to the Town Hall, bounded to the northeast by Exchange Street East and to the southwest by High Street.'], 'thought_3': 'The search results confirm that Liverpool is located in the north of England.', 'tool_name_3': 'search', 'tool_args_3': {'query': 'Liverpool location in England'}, 'observation_3': ['Liverpool | Liverpool ( ) is a city in North West England, with an estimated population of 478,580 in 2015. With its surrounding areas, it is the fifth-largest metropolitan area in the UK, with over 2.24 million people in 2011. The local authority is Liverpool City Council, the most populous local government district within the metropolitan county of Merseyside and the largest within the Liverpool City Region.', 'Liverpool (disambiguation) | Liverpool is a city in Merseyside, England.', 'Liverpool, London and Globe Building | The Liverpool, London and Globe Building is located in Dale Street, Liverpool, Merseyside, England. It fills a block adjacent to the Town Hall, bounded to the northeast by Exchange Street East and to the southwest by High Street.'], 'thought_4': 'The search results confirm that Liverpool is located in the north of England.', 'tool_name_4': 'search', 'tool_args_4': {'query': 'Liverpool location in England'}, 'observation_4': ['Liverpool | Liverpool ( ) is a city in North West England, with an estimated population of 478,580 in 2015. With its surrounding areas, it is the fifth-largest metropolitan area in the UK, with over 2.24 million people in 2011. The local authority is Liverpool City Council, the most populous local government district within the metropolitan county of Merseyside and the largest within the Liverpool City Region.', 'Liverpool (disambiguation) | Liverpool is a city in Merseyside, England.', 'Liverpool, London and Globe Building | The Liverpool, London and Globe Building is located in Dale Street, Liverpool, Merseyside, England. It fills a block adjacent to the Town Hall, bounded to the northeast by Exchange Street East and to the southwest by High Street.']},\n",
       "    reasoning='The search results confirm that Liverpool is located in the north of England.',\n",
       "    answer='northern'\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5500/static-files/lib/notebook-trace-renderer/index.html?trace_id=5c3423a631a342bbadb47132e1fa360e&amp;experiment_id=302620010842219766&amp;version=2.22.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(request_id=5c3423a631a342bbadb47132e1fa360e)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaded_optimize_react = dspy.ReAct(\"question -> answer\", tools=[search])\n",
    "loaded_optimize_react.load(\"optimized_react.json\")\n",
    "\n",
    "loaded_optimize_react(question=\"Is Liverpool found in northern or southern England?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449c66e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
