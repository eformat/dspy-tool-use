{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e97d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/git/dspy-tool-use/venv/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5500\")\n",
    "mlflow.set_experiment(\"rag\")\n",
    "mlflow.dspy.autolog(\n",
    "    log_compiles=True,    # Track optimization process\n",
    "    log_evals=True,       # Track evaluation results\n",
    "    log_traces_from_compile=True  # Track program traces during optimization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b09ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import openai\n",
    "import os\n",
    "\n",
    "LLM_URL=os.getenv('LLM_URL', 'http://localhost:8080/v1')\n",
    "API_KEY=os.getenv('API_KEY', 'fake')\n",
    "LLM_MODEL=os.getenv('LLM_MODEL', 'openai/models/Llama-3.2-3B-Instruct-Q8_0.gguf')\n",
    "MAX_TOKENS=os.getenv('MAX_TOKENS', 6000)\n",
    "TEMPERATURE=os.getenv('TEMPERATURE', 0.2)\n",
    "dspy.enable_logging()\n",
    "lm = dspy.LM(model=LLM_MODEL,\n",
    "             api_base=LLM_URL,  # ensure this points to your port\n",
    "             api_key=API_KEY,\n",
    "             temperature=TEMPERATURE,\n",
    "             model_type='chat',\n",
    "             stream=False)\n",
    "dspy.configure(lm=lm)\n",
    "#dspy.settings.configure(track_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8da216e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:37:44 - LiteLLM:INFO\u001b[0m: utils.py:2991 - \n",
      "LiteLLM completion() model= Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= Llama-3.2-3B-Instruct-Q8_0.gguf; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8080/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:37:47 - LiteLLM:INFO\u001b[0m: utils.py:1213 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:37:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:37:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:37:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:37:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:37:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:37:47 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5500/static-files/lib/notebook-trace-renderer/index.html?trace_id=d25a6d2ee5694ab4a14fc5f36003b094&amp;experiment_id=414799578984116612&amp;version=2.22.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(request_id=d25a6d2ee5694ab4a14fc5f36003b094)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m10:40:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:40:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:40:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:40:01 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:42:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:42:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:42:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:42:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:43:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:43:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:43:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:43:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:43:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:43:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:43:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:43:49 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:45:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:45:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:45:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:45:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:45:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:45:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:45:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:45:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:52:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:52:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:52:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:52:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:52:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:52:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:52:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: openai/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "\u001b[92m10:52:21 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n",
      "INFO:LiteLLM:selected model name for cost calculation: Llama-3.2-3B-Instruct-Q8_0.gguf\n"
     ]
    }
   ],
   "source": [
    "qa = dspy.Predict('question: str -> response: str')\n",
    "response = qa(question=\"what are high memory and low memory on linux?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39b1414a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-06-03T10:37:47.458714]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `question` (str)\n",
      "Your output fields are:\n",
      "1. `response` (str)\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## response ## ]]\n",
      "{response}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `question`, produce the fields `response`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## question ## ]]\n",
      "what are high memory and low memory on linux?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## response ## ]]\n",
      "High memory and low memory are two terms used to describe the amount of free memory available on a Linux system. High memory refers to a system with a significant amount of free memory, typically above 50% of the total system memory. This allows for smooth performance, as the system can handle multiple tasks and applications without running out of memory.\n",
      "\n",
      "Low memory, on the other hand, refers to a system with limited free memory, typically below 10% of the total system memory. This can cause performance issues, as the system may need to swap memory to disk, leading to slower performance and potential crashes.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba842ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning='Red Hat Linux is a popular open-source operating system based on the Linux kernel. It is developed and maintained by Red Hat, a company that provides support, services, and subscriptions to users. Red Hat Linux is known for its stability, security, and ease of use, making it a popular choice for servers, desktops, and mobile devices.',\n",
       "    response='Red Hat Linux is a variant of the Linux operating system that is designed to be highly stable, secure, and easy to use. It is available in several different editions, including Red Hat Enterprise Linux (RHEL), Red Hat Enterprise Linux Server (RHES), and Red Hat Enterprise Linux Workstation (RHELW). Red Hat Linux is widely used in enterprise environments, as well as by individuals and organizations of all sizes.'\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5500/static-files/lib/notebook-trace-renderer/index.html?trace_id=3479890ea07b42b4a423d4d057faed21&amp;experiment_id=414799578984116612&amp;version=2.22.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(request_id=3479890ea07b42b4a423d4d057faed21)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cot = dspy.ChainOfThought('question -> response')\n",
    "cot(question=\"what is red hat linux?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20093700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson\n",
    "from dspy.utils import download\n",
    "\n",
    "# Download question--answer pairs from the RAG-QA Arena \"Tech\" dataset.\n",
    "download(\"https://huggingface.co/dspy/cache/resolve/main/ragqa_arena_tech_examples.jsonl\")\n",
    "\n",
    "with open(\"ragqa_arena_tech_examples.jsonl\") as f:\n",
    "    data = [ujson.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02687482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'why igp is used in mpls?',\n",
       " 'response': \"An IGP exchanges routing prefixes between gateways/routers.  \\nWithout a routing protocol, you'd have to configure each route on every router and you'd have no dynamic updates when routes change because of link failures. \\nFuthermore, within an MPLS network, an IGP is vital for advertising the internal topology and ensuring connectivity for MP-BGP inside the network.\",\n",
       " 'gold_doc_ids': [2822, 2823]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect one datapoint.\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaa81e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': 'why are my text messages coming up as maybe?', 'response': 'This is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". \\n\\nHowever, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled.', 'gold_doc_ids': [3956, 3957, 8034]}) (input_keys={'question'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [dspy.Example(**d).with_inputs('question') for d in data]\n",
    "\n",
    "# Let's pick an `example` here from the data.\n",
    "example = data[2]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d17dcc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 100, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.Random(0).shuffle(data)\n",
    "# 200, 200:500, 500:1000\n",
    "trainset, devset, testset = data[:50], data[50:150], data[150:450]\n",
    "\n",
    "len(trainset), len(devset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "120bd62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'question': 'why are my text messages coming up as maybe?', 'response': 'This is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". \\n\\nHowever, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled.', 'gold_doc_ids': [3956, 3957, 8034]}) (input_keys={'question'})\n",
      "Prediction(\n",
      "    reasoning='Your text messages are being flagged as \"maybe\" because our system is programmed to flag messages that don\\'t contain a clear question or statement. This is a precautionary measure to ensure that we don\\'t send unsolicited messages to users. However, it\\'s possible that the message you\\'re referring to is a legitimate question or statement that was misinterpreted.',\n",
      "    response='To resolve this issue, you can try rephrasing your message to make it clearer. For example, you could rephrase \"why are my text messages coming up as maybe?\" to \"I\\'m getting messages marked as \\'maybe\\' - can you help me understand why?\" This should help our system to better understand your question and provide a more accurate response.'\n",
      ")\n",
      "Question: \t why are my text messages coming up as maybe?\n",
      "\n",
      "Gold Response: \t This is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". \n",
      "\n",
      "However, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled.\n",
      "\n",
      "Predicted Response: \t To resolve this issue, you can try rephrasing your message to make it clearer. For example, you could rephrase \"why are my text messages coming up as maybe?\" to \"I'm getting messages marked as 'maybe' - can you help me understand why?\" This should help our system to better understand your question and provide a more accurate response.\n",
      "\n",
      "Semantic F1 Score: 0.67\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5500/static-files/lib/notebook-trace-renderer/index.html?trace_id=55cd646fd2d54cae8bd9acce5a7e0543&amp;experiment_id=414799578984116612&amp;trace_id=fd4f5b5b378a489bb742ab9963018f36&amp;experiment_id=414799578984116612&amp;version=2.22.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(request_id=55cd646fd2d54cae8bd9acce5a7e0543), Trace(request_id=fd4f5b5b378a489bb742ab9963018f36)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dspy.evaluate import SemanticF1\n",
    "\n",
    "# Instantiate the metric.\n",
    "metric = SemanticF1(decompositional=True)\n",
    "\n",
    "# Produce a prediction from our `cot` module, using the `example` above as input.\n",
    "pred = cot(**example.inputs())\n",
    "\n",
    "print(example)\n",
    "print(pred)\n",
    "\n",
    "# Compute the metric score for the prediction.\n",
    "score = metric(example, pred)\n",
    "\n",
    "print(f\"Question: \\t {example.question}\\n\")\n",
    "print(f\"Gold Response: \\t {example.response}\\n\")\n",
    "print(f\"Predicted Response: \\t {pred.response}\\n\")\n",
    "print(f\"Semantic F1 Score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44cf6ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-06-03T12:00:11.444069]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `question` (str)\n",
      "2. `ground_truth` (str)\n",
      "3. `system_response` (str)\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `ground_truth_key_ideas` (str): enumeration of key ideas in the ground truth\n",
      "3. `system_response_key_ideas` (str): enumeration of key ideas in the system response\n",
      "4. `discussion` (str): discussion of the overlap between ground truth and system response\n",
      "5. `recall` (float): fraction (out of 1.0) of ground truth covered by the system response\n",
      "6. `precision` (float): fraction (out of 1.0) of system response covered by the ground truth\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## ground_truth ## ]]\n",
      "{ground_truth}\n",
      "\n",
      "[[ ## system_response ## ]]\n",
      "{system_response}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## ground_truth_key_ideas ## ]]\n",
      "{ground_truth_key_ideas}\n",
      "\n",
      "[[ ## system_response_key_ideas ## ]]\n",
      "{system_response_key_ideas}\n",
      "\n",
      "[[ ## discussion ## ]]\n",
      "{discussion}\n",
      "\n",
      "[[ ## recall ## ]]\n",
      "{recall}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## precision ## ]]\n",
      "{precision}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Compare a system's response to the ground truth to compute recall and precision of key ideas.\n",
      "        You will first enumerate key ideas in each response, discuss their overlap, and then report recall and precision.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## question ## ]]\n",
      "why are my text messages coming up as maybe?\n",
      "\n",
      "[[ ## ground_truth ## ]]\n",
      "This is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". \n",
      "\n",
      "However, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled.\n",
      "\n",
      "[[ ## system_response ## ]]\n",
      "To resolve this issue, you can try rephrasing your message to make it clearer. For example, you could rephrase \"why are my text messages coming up as maybe?\" to \"I'm getting messages marked as 'maybe' - can you help me understand why?\" This should help our system to better understand your question and provide a more accurate response.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## ground_truth_key_ideas ## ]]`, then `[[ ## system_response_key_ideas ## ]]`, then `[[ ## discussion ## ]]`, then `[[ ## recall ## ]]` (must be formatted as a valid Python float), then `[[ ## precision ## ]]` (must be formatted as a valid Python float), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The system response attempts to resolve the issue by suggesting a rephrased version of the question, which should help the system better understand the user's query and provide a more accurate response.\n",
      "\n",
      "[[ ## ground_truth_key_ideas ## ]]\n",
      "* Proactivity feature in iOS 9\n",
      "* Bug in iOS 11.2\n",
      "* \"Maybe\" label in text messages\n",
      "* \"Find Contacts in Other Apps\" feature\n",
      "\n",
      "[[ ## system_response_key_ideas ## ]]\n",
      "* Rephrase the question to make it clearer\n",
      "* Improve system understanding\n",
      "* Provide a more accurate response\n",
      "\n",
      "[[ ## discussion ## ]]\n",
      "The system response discusses the potential bug in iOS 11.2 and suggests a rephrased version of the question to help the system better understand the user's query. This approach aims to improve the accuracy of the response and provide a more helpful solution.\n",
      "\n",
      "[[ ## recall ## ]]\n",
      "0.5\n",
      "\n",
      "[[ ## precision ## ]]\n",
      "1.0\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44fad027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.41 / 34 (45.3%):  34%|███▍      | 34/100 [00:02<00:03, 16.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 12:08:25 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 49.38 / 100 (49.4%): 100%|██████████| 100/100 [00:05<00:00, 18.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/03 12:08:28 INFO dspy.evaluate.evaluate: Average Metric: 49.38247399466397 / 100 (49.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval at: http://localhost:5500/#/experiments/414799578984116612/runs/e8923ca5fe75402f9ffe2db0d82b668d\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/414799578984116612\n",
      "🏃 View run rag_evaluation at: http://localhost:5500/#/experiments/414799578984116612/runs/e0a6ff97574c44aeb04efdb18da2c037\n",
      "🧪 View experiment at: http://localhost:5500/#/experiments/414799578984116612\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5500/static-files/lib/notebook-trace-renderer/index.html?trace_id=5ebb3a267a2e4f789f4365412c097290&amp;experiment_id=414799578984116612&amp;trace_id=fbc1802e78d34cfeb0f7583f01617a1d&amp;experiment_id=414799578984116612&amp;trace_id=0422cc2639ca4732b9bc8b164e01d311&amp;experiment_id=414799578984116612&amp;trace_id=24773a9289ca49b3ae290c703d52df61&amp;experiment_id=414799578984116612&amp;trace_id=8501469160ea43db8bc64f64c631de10&amp;experiment_id=414799578984116612&amp;trace_id=32ef0a67d03948df864baed6f9d9332f&amp;experiment_id=414799578984116612&amp;trace_id=5f722d7e5464466c9a85f192b0f62c32&amp;experiment_id=414799578984116612&amp;trace_id=8267e9b82b1f47bc959e4c8c46b1e20a&amp;experiment_id=414799578984116612&amp;trace_id=594d850810a747a8b494eb702d758a2e&amp;experiment_id=414799578984116612&amp;trace_id=d8baf1b8876a43b9bd6eb51bd8d84f41&amp;experiment_id=414799578984116612&amp;version=2.22.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(request_id=5ebb3a267a2e4f789f4365412c097290), Trace(request_id=fbc1802e78d34cfeb0f7583f01617a1d), Trace(request_id=0422cc2639ca4732b9bc8b164e01d311), Trace(request_id=24773a9289ca49b3ae290c703d52df61), Trace(request_id=8501469160ea43db8bc64f64c631de10), Trace(request_id=32ef0a67d03948df864baed6f9d9332f), Trace(request_id=5f722d7e5464466c9a85f192b0f62c32), Trace(request_id=8267e9b82b1f47bc959e4c8c46b1e20a), Trace(request_id=594d850810a747a8b494eb702d758a2e), Trace(request_id=d8baf1b8876a43b9bd6eb51bd8d84f41)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run(run_name=\"rag_evaluation\"):\n",
    "    evaluate = dspy.Evaluate(\n",
    "        devset=devset,\n",
    "        metric=metric,\n",
    "        num_threads=24,\n",
    "        display_progress=True,\n",
    "        # To record the outputs and detailed scores to MLflow\n",
    "        return_all_scores=True,\n",
    "        return_outputs=True,\n",
    "    )\n",
    "\n",
    "    # Evaluate the program as usual\n",
    "    aggregated_score, outputs, all_scores = evaluate(cot)\n",
    "\n",
    "\n",
    "    # Log the aggregated score\n",
    "    mlflow.log_metric(\"semantic_f1_score\", aggregated_score)\n",
    "    # Log the detailed evaluation results as a table\n",
    "    mlflow.log_table(\n",
    "        {\n",
    "            \"Question\": [example.question for example in devset],\n",
    "            \"Gold Response\": [example.response for example in devset],\n",
    "            \"Predicted Response\": outputs,\n",
    "            \"Semantic F1 Score\": all_scores,\n",
    "        },\n",
    "        artifact_file=\"eval_results.json\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20944e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an evaluator that we can re-use (non-mlflow)\n",
    "#evaluate = dspy.Evaluate(devset=devset, metric=metric, num_threads=24,\n",
    "#                         display_progress=True, display_table=2)\n",
    "\n",
    "# Evaluate the Chain-of-Thought program.\n",
    "#evaluate(cot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
